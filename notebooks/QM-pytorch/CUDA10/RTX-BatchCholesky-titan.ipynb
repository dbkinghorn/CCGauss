{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution device:  cuda:0\n",
      "PyTorch version:  1.0.0\n",
      "CUDA available:  True\n",
      "CUDA version:  10.0.130\n",
      "CUDA device: Graphics Device\n"
     ]
    }
   ],
   "source": [
    "dtype = th.float32\n",
    "\n",
    "gpuid = 0\n",
    "device = th.device(\"cuda:\"+ str(gpuid))\n",
    "#device = th.device(\"cpu\")\n",
    "\n",
    "print(\"Execution device: \",device)\n",
    "print(\"PyTorch version: \", th.__version__ )\n",
    "print(\"CUDA available: \", th.cuda.is_available())\n",
    "print(\"CUDA version: \", th.version.cuda)\n",
    "print(\"CUDA device:\", th.cuda.get_device_name(gpuid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "# Batched vech2L input V is nb x n(n+1)/2\n",
    "def bvech2L(V,nb,n):\n",
    "    count = 0\n",
    "    L = th.zeros((nb,n,n))\n",
    "    for j in range(n):\n",
    "        for i in range(j,n):\n",
    "            L[...,i,j]=V[...,count]\n",
    "            count = count + 1\n",
    "    return th.tensor(L , device=device, dtype=dtype)\n",
    "\n",
    "# Batched Cholesky decomp\n",
    "def cholesky(A):\n",
    "    L = th.zeros_like(A)\n",
    "\n",
    "    for i in range(A.shape[-1]):\n",
    "        for j in range(i+1):\n",
    "            s = 0.0\n",
    "            for k in range(j):\n",
    "                s = s + L[...,i,k].clone() * L[...,j,k].clone()\n",
    "\n",
    "            L[...,i,j] = th.sqrt(A[...,i,i] - s) if (i == j) else \\\n",
    "                      (1.0 / L[...,j,j].clone() * (A[...,i,j] - s))\n",
    "    return L\n",
    "\n",
    "# Batched inverse of lower triangular matrices\n",
    "def inverseL(L):\n",
    "    n = L.shape[-1]\n",
    "    invL = th.zeros_like(L)\n",
    "    for j in range(0,n):\n",
    "        invL[...,j,j] = 1.0/L[...,j,j]\n",
    "        for i in range(j+1,n):\n",
    "            S = 0.0\n",
    "            for k in range(i+1):\n",
    "                S = S - L[...,i,k]*invL[...,k,j].clone()\n",
    "            invL[...,i,j] = S/L[...,i,i]\n",
    "\n",
    "    return invL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(n=4,m=1000):\n",
    "\n",
    "    nn = int(n*(n+1)/2)\n",
    "\n",
    "    th.manual_seed(42)\n",
    "    X = th.rand((m,nn), device=device, dtype=dtype)\n",
    "    L = th.add(bvech2L(X,m,n),th.tensor(th.eye(n), device=device, dtype=dtype))\n",
    "    A = th.matmul(L,th.transpose(L, 1, 2))\n",
    "    print(\"Shape of A {}\".format(A.shape))\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    cholA = th.zeros_like(A)\n",
    "    for i in range(m):\n",
    "        cholA[i,:,:] = th.potrf(A[i], upper=False)\n",
    "\n",
    "    runtime = time.time() - start_time\n",
    "    print(\"loop version took {} seconds \".format(runtime))\n",
    "    return runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(n=4,m=1000):\n",
    "\n",
    "    nn = int(n*(n+1)/2)\n",
    "\n",
    "    th.manual_seed(42)\n",
    "    X = th.rand((m,nn), device=device, dtype=dtype)\n",
    "    L = th.add(bvech2L(X,m,n), th.tensor(th.eye(n), device=device, dtype=dtype))\n",
    "    A = th.matmul(L,th.transpose(L, 1, 2))\n",
    "    print(\"Shape of A {}\".format(A.shape))\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    cholA = th.zeros_like(A)\n",
    "\n",
    "    cholA = cholesky(A)\n",
    "\n",
    "    runtime = time.time() - start_time\n",
    "    print(\"batched version took {} seconds \".format(runtime))\n",
    "    return runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kinghorn/anaconda3/envs/pytorch10/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/kinghorn/anaconda3/envs/pytorch10/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "/home/kinghorn/anaconda3/envs/pytorch10/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: torch.potrf is deprecated in favour of torch.cholesky and will be removed in the next release. Please use torch.cholesky instead and note that the :attr:`upper` argument in torch.cholesky defaults to ``False``.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A torch.Size([10000, 10, 10])\n",
      "loop version took 21.534914255142212 seconds \n"
     ]
    }
   ],
   "source": [
    "tval = test_loop(n=10,m=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A torch.Size([10000, 10, 10])\n",
      "batched version took 0.014473438262939453 seconds \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kinghorn/anaconda3/envs/pytorch10/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/kinghorn/anaconda3/envs/pytorch10/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "tval = test_batch(n=10,m=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A torch.Size([1000000, 10, 10])\n",
      "batched version took 0.008554220199584961 seconds \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kinghorn/anaconda3/envs/pytorch10/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/kinghorn/anaconda3/envs/pytorch10/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "tval = test_batch(n=10,m=1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kinghorn/anaconda3/envs/pytorch10/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A torch.Size([8000000, 10, 10])\n",
      "batched version took 0.05316901206970215 seconds \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kinghorn/anaconda3/envs/pytorch10/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "tval = test_batch(n=10,m=8000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch10",
   "language": "python",
   "name": "pytorch10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
