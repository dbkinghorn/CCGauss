{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cgauss in PyTorch\n",
    "Correlated gaussian basis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "import time\n",
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution device:  cpu\n",
      "PyTorch version:  1.1.0\n",
      "CUDA available:  True\n",
      "CUDA version:  10.0.130\n",
      "CUDA device: TITAN V\n"
     ]
    }
   ],
   "source": [
    "dtype = th.float64\n",
    "\n",
    "gpuid = 0\n",
    "#device = th.device(\"cuda:\"+ str(gpuid))\n",
    "device = th.device(\"cpu\")\n",
    "\n",
    "print(\"Execution device: \",device)\n",
    "print(\"PyTorch version: \", th.__version__ )\n",
    "print(\"CUDA available: \", th.cuda.is_available())\n",
    "print(\"CUDA version: \", th.version.cuda)\n",
    "print(\"CUDA device:\", th.cuda.get_device_name(gpuid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def vech2L(v,n):\n",
    "    count = 0\n",
    "    L = th.zeros((n,n), device=device, dtype=dtype)\n",
    "    for j in range(n):\n",
    "        for i in range(j,n):\n",
    "            L[i,j]=v[count]\n",
    "            count = count + 1\n",
    "    return L \n",
    "\n",
    "# batched vech2L input is \"X\" as V nb x n(n+1)/2\n",
    "def bvech2L(V,nb,n):\n",
    "    count = 0\n",
    "    L = th.zeros((nb,n,n), device=device, dtype=dtype)\n",
    "    for j in range(n):\n",
    "        for i in range(j,n):\n",
    "            L[...,i,j]=V[...,count]\n",
    "            count = count + 1\n",
    "    return L + th.eye(n, device=device, dtype=dtype)\n",
    "    #return L\n",
    "\n",
    "# Batched Cholesky decomp\n",
    "def cholesky(A):\n",
    "    L = th.zeros_like(A)\n",
    "    \n",
    "    for i in range(A.shape[-1]):\n",
    "        for j in range(i+1):\n",
    "            s = 0.0\n",
    "            for k in range(j):\n",
    "                s = s + L[...,i,k].clone() * L[...,j,k].clone()\n",
    "            \n",
    "            L[...,i,j] = th.sqrt(A[...,i,i] - s) if (i == j) else \\\n",
    "                      (1.0 / L[...,j,j].clone() * (A[...,i,j] - s))\n",
    "    return L\n",
    "\n",
    "# Batched inverse of lower triangular matrices \n",
    "def inverseL(L):\n",
    "    n = L.shape[-1]\n",
    "    invL = th.zeros_like(L)\n",
    "    for j in range(0,n):\n",
    "        invL[...,j,j] = 1.0/L[...,j,j]\n",
    "        for i in range(j+1,n):\n",
    "            S = 0.0\n",
    "            for k in range(i+1):\n",
    "                S = S - L[...,i,k]*invL[...,k,j].clone()\n",
    "            invL[...,i,j] = S/L[...,i,i]\n",
    "\n",
    "    return invL\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_energyrc(x,n,nb,Mass,Qmat,Sym,symc):\n",
    "    \n",
    "    nx = len(x);\n",
    "    nn = int(n*(n+1)/2);\n",
    "    nsym = len(symc);\n",
    "    \n",
    "    # extract linear coefs \"eigen vector\"\n",
    "    c = x[-nb:];\n",
    "    # reshape non-linear variables for easier indexing\n",
    "    X = th.reshape(x[:nb*nn], (nb,nn))\n",
    "    \n",
    "    # generate tensor of lower triangular matrices from X\n",
    "    # these are the non-linear parameters of the basis set\n",
    "    L = th.zeros((nb,n,n), device=device, dtype=dtype)\n",
    "    L = bvech2L(X,nb,n)\n",
    "    \n",
    "    # get the determinates for L |L| is the product of diag elements\n",
    "    detL = th.abs(th.prod(th.diagonal(L, offset=0, dim1=-1, dim2=-2),1))\n",
    "    \n",
    "    # create the tensor of matrix products of the L matrices AKL = L x Ltranspose\n",
    "    AK = th.matmul(L,th.transpose(L, 1, 2))\n",
    "\n",
    "    \n",
    "    # Initialize H T V and S matrices\n",
    "    # H = T + V, we are solving (H-ES)c = 0 for E (energy)\n",
    "    H = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    S = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    T = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    V = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    \n",
    "\n",
    "    # outer loop is over symmetry terms, the matrices are summed over these sym terms\n",
    "    for k in range(0,nsym):\n",
    "        \n",
    "        P = Sym[k,:,:]\n",
    "        # symetry projection is applied only to \"ket\" this constructs AL\n",
    "        AL = th.matmul(th.t(P), th.matmul(AK,P))\n",
    "\n",
    "        # Akl = Ak + Al\n",
    "        AKL = th.zeros((nb,nb,n,n), device=device, dtype=dtype)\n",
    "        #for i in range(nb):\n",
    "        #    for j in range(nb):\n",
    "        #        #AKL[i,j] =  th.add(AK[i], AL[j])\n",
    "        #        AKL[i,j] =  AK[i] + AL[j]\n",
    "        AKL = AL.repeat((nb,1,1,1)) + th.transpose(AK.repeat((nb,1,1,1)), 0,1)\n",
    "        \n",
    "        # get the Cholesky decomp of all Akl martices\n",
    "        cholAKL = cholesky(AKL)\n",
    "        \n",
    "        # get determinates of AKL from diags |Akl|= |Lk|**2\n",
    "        detAKL = th.prod(th.diagonal(cholAKL, offset=0, dim1=-1, dim2=-2),-1)**2\n",
    "        \n",
    "        # compute inverses of lower tringular matrices in cholAKL\n",
    "        invLKL = inverseL(cholAKL)\n",
    "        \n",
    "        # inverses Akl^-1 = Lkl' x Lkl\n",
    "        invAKL = th.matmul(th.transpose(invLKL, dim0=-1, dim1=-2),invLKL)\n",
    "\n",
    "        # get terms needed for potential energy V\n",
    "        RIJ = th.zeros_like(invAKL, device=device, dtype=dtype);\n",
    "        # 1/rij i~=j\n",
    "        for j in range(0,n-1):\n",
    "            for i in range(j+1,n):\n",
    "                tmp2 = invAKL[...,i,i] + invAKL[...,j,j] - 2*invAKL[...,i,j];\n",
    "                RIJ[...,i,j] = th.rsqrt(tmp2)\n",
    "\n",
    "        # 1/rij i=j\n",
    "        for i in range(0,n):\n",
    "            RIJ[...,i,i] = th.rsqrt(invAKL[...,i,i])    \n",
    "\n",
    "        # MATRIX ELEMENTS\n",
    "        \n",
    "        # Overlap: (normalized)\n",
    "        # Skl = 2^3n/2 (||Lk|| ||Ll||/|AKL|)^3/2\n",
    "        SKL = 2**(n*1.5) * th.sqrt( th.pow(th.ger(detL, detL)/detAKL ,3) );\n",
    "\n",
    "        # Kinetic energy\n",
    "        #TKL = SKL*(6*th.trace(Mass@Ak@invAkl@Al)) = skl*(6*th.sum(Mass*(Ak@invAkl@Al)))\n",
    "\n",
    "        Tmat = th.zeros_like(invAKL)\n",
    "        #for i in range(nb):\n",
    "        #    for j in range(nb):\n",
    "        #        Tmat[i,j] = (AK[i]@invAKL[i,j]@AL[j])\n",
    "        Tmat = th.matmul(th.transpose(AK.repeat((nb,1,1,1)), 0,1), th.matmul(invAKL,AL))\n",
    "        TKL = 6*SKL*th.sum(Mass*Tmat, dim=(-2,-1))\n",
    "\n",
    "        # potential energy\n",
    "        TWOoSqrtPI = 1.1283791670955126 # 2/sqrt(pi)\n",
    "        \n",
    "        VKL = TWOoSqrtPI*SKL*th.sum(RIJ*Qmat, dim=(-2,-1))\n",
    "    \n",
    "        # accumulate matrices over sym terms\n",
    "        S = S + symc[k]*SKL\n",
    "        T = T + symc[k]*TKL\n",
    "        V = V + symc[k]*VKL\n",
    "        \n",
    "    # Hamiltonian\n",
    "    H = T + V\n",
    "    \n",
    "    # complete lower triangle of H and S\n",
    "    #for i in range(0,nb):\n",
    "    #    for j in range(i+1,nb):\n",
    "    #        H[j,i] = H[i,j]\n",
    "    #        S[j,i] = S[i,j]\n",
    "    #        #H[i,j] = H[j,i];\n",
    "    #        #S[i,j] = S[j,i];\n",
    "    H = th.triu(H,1)+th.t(th.triu(H))\n",
    "    S = th.triu(S,1)+th.t(th.triu(S))\n",
    "    # compute Rayleigh quotent (it is the smallest energy eigen value when minimized over c)\n",
    "    cHc = c@H@c;\n",
    "    cSc = c@S@c;\n",
    "    eng = cHc/cSc;\n",
    "    \n",
    "    return eng           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_energyrc(steps=1, num_basis=8, restart=True):\n",
    "    \n",
    "    #\n",
    "    # Li BO setup\n",
    "    #\n",
    "    n=3;\n",
    "    \n",
    "    Mass = th.tensor([[0.5, 0.0, 0.0],\n",
    "                     [0.0, 0.5, 0.0],\n",
    "                     [0.0, 0.0, 0.5]], device=device, dtype=dtype);\n",
    "    \n",
    "    Charge = th.tensor([-3, 1, 1, -3, 1, -3], device=device, dtype=dtype);\n",
    "    Charge = vech2L(Charge,n)\n",
    "    \n",
    "    # symmetry projection terms\n",
    "    Sym = th.zeros((6,3,3), device=device, dtype=dtype)\n",
    "    # (1)(2)(3)\n",
    "    Sym[0,:,:] = th.tensor([[1,0,0],[0,1,0],[0,0,1]], device=device, dtype=dtype);\n",
    "    # (12)\n",
    "    Sym[1,:,:] = th.tensor([[0,1,0],[1,0,0],[0,0,1]], device=device, dtype=dtype);\n",
    "    # (13)\n",
    "    Sym[2,:,:] = th.tensor([[0,0,1],[0,1,0],[1,0,0]], device=device, dtype=dtype);\n",
    "    # (23)\n",
    "    Sym[3,:,:] = th.tensor([[1,0,0],[0,0,1],[0,1,0]], device=device, dtype=dtype);\n",
    "    # (123)\n",
    "    Sym[4,:,:] = th.tensor([[0,1,0],[0,0,1],[1,0,0]], device=device, dtype=dtype);\n",
    "    # (132)\n",
    "    Sym[5,:,:] = th.tensor([[0,0,1],[1,0,0],[0,1,0]], device=device, dtype=dtype);\n",
    "\n",
    "    # coeff's\n",
    "    symc = th.tensor([4.0,4.0,-2.0,-2.0,-2.0,-2.0], device=device, dtype=dtype);\n",
    "\n",
    "    # Sample parameters should return energy of -7.3615\n",
    "    xvechL=th.tensor([\n",
    "         1.6210e+00, -2.1504e-01,  9.0755e-01,  9.7866e-01, -2.8418e-01,\n",
    "        -3.5286e+00, -3.3045e+00, -4.5036e+00, -3.2116e-01, -7.1901e-02,\n",
    "         1.5167e+00, -8.4489e-01, -2.1377e-01, -3.6127e-03, -5.3774e-03,\n",
    "        -2.1263e+00, -2.5191e-01,  2.1235e+00, -2.1396e-01, -1.4084e-03,\n",
    "        -1.0092e-02,  4.5349e+00,  9.4837e-03,  1.1225e+00, -2.1315e-01,\n",
    "         5.8451e-02, -4.9410e-03,  5.0853e+00,  7.3332e-01,  5.0672e+00,\n",
    "        -2.1589e-01, -6.8986e-03, -1.4310e-02,  1.5979e+00,  3.3946e-02,\n",
    "        -8.7965e-01, -1.1121e+00, -2.1903e-03, -4.6925e-02,  2.1457e-01,\n",
    "         3.3045e-03,  4.5120e+00, -2.1423e-01, -1.6493e-02, -2.3429e-03,\n",
    "        -8.6715e-01, -6.7070e-02,  1.5998e+00\n",
    "     ], device=device, dtype=dtype, requires_grad=False)\n",
    "\n",
    "    evec = th.tensor([\n",
    "      -6.0460e-02,  7.7708e-05, 1.6152e+00,  9.5443e-01,  \n",
    "      1.1771e-01,  3.2196e+00,  9.6344e-01, 3.1398e+00\n",
    "    ], device=device, dtype=dtype, requires_grad=False)\n",
    "\n",
    "    \n",
    "    # uncomment following lines to test above \n",
    "    #nb=8\n",
    "    #x1 = th.tensor(th.cat((xvechL,evec)), device=device, dtype=dtype, requires_grad=True)\n",
    "    #energy = b_energyrc(x1,n,nb,Mass,Charge,Sym,symc) \n",
    "    #print(energy) # should be -7.3615\n",
    "    #return x1\n",
    "    \n",
    "    if restart:\n",
    "        nb=num_basis\n",
    "        x1 = xrestart\n",
    "    else:\n",
    "        # random start point\n",
    "        nb=num_basis\n",
    "        #th.manual_seed(333)\n",
    "        x1 = th.empty(int(nb*n*(n+1)/2 + nb), device=device, dtype=dtype, requires_grad=True)\n",
    "        th.nn.init.uniform_(x1, a=-0.8, b=0.8)\n",
    "        \n",
    "    # start from a restart value\n",
    "    #x1 = xrestart\n",
    "    #print(energy)\n",
    "    #return x1\n",
    "    \n",
    "    # Do the Optimization\n",
    "    #optimizer = th.optim.LBFGS([x1])\n",
    "    #optimizer = th.optim.Adadelta([x1], lr=160.0)\n",
    "    #optimizer = th.optim.Adam([x1], lr=0.00005)\n",
    "    optimizer = th.optim.Rprop([x1], lr=0.00001, etas=(0.5, 1.2), step_sizes=(1e-07, 50))\n",
    "    \n",
    "    #scheduler = th.optim.lr_scheduler.ReduceLROnPlateau(optimizer,threshold=0.00001,cooldown=3, verbose=True,patience=2, factor=0.5)\n",
    "    \n",
    "    for i in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        loss = b_energyrc(x1,n,nb,Mass,Charge,Sym,symc)\n",
    "        loss.backward()\n",
    "        #def closure():\n",
    "        #    return b_energyrc(x1,n,nb,Mass,Charge,Sym,symc)\n",
    "        #optimizer.step(closure)\n",
    "        optimizer.step()\n",
    "        #scheduler.step(loss)\n",
    "        \n",
    "        if (i<20 or not i%10):print('step: {:5}  f: {:4.12f}  gradNorm: {:.9f}'.format(i, loss, th.norm(x1.grad)))\n",
    "    # print last value\n",
    "    print('step: {:5}  f: {:4.12f}  gradNorm: {:.9f}'.format(i, loss, th.norm(x1.grad)))\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization restart: 0\n",
      "step:     0  f: -2.722755628079  gradNorm: 2.496505147\n",
      "step:     1  f: -2.723696924819  gradNorm: 2.496361632\n",
      "step:     2  f: -2.724826461609  gradNorm: 2.496188766\n",
      "step:     3  f: -2.726181876599  gradNorm: 2.495980473\n",
      "step:     4  f: -2.727808330784  gradNorm: 2.495729643\n",
      "step:     5  f: -2.729760010378  gradNorm: 2.495428338\n",
      "step:     6  f: -2.732101929465  gradNorm: 2.495066460\n",
      "step:     7  f: -2.734912090929  gradNorm: 2.494633058\n",
      "step:     8  f: -2.738284092250  gradNorm: 2.494124476\n",
      "step:     9  f: -2.742330219225  gradNorm: 2.493512808\n",
      "step:    10  f: -2.747185110611  gradNorm: 2.492766650\n",
      "step:    11  f: -2.753010286423  gradNorm: 2.491873956\n",
      "step:    12  f: -2.759999456414  gradNorm: 2.490800469\n",
      "step:    13  f: -2.768384743626  gradNorm: 2.489491938\n",
      "step:    14  f: -2.778444555254  gradNorm: 2.487957840\n",
      "step:    15  f: -2.790512805913  gradNorm: 2.486131311\n",
      "step:    16  f: -2.804988672174  gradNorm: 2.483882529\n",
      "step:    17  f: -2.822349022934  gradNorm: 2.481085172\n",
      "step:    18  f: -2.843164391180  gradNorm: 2.477691866\n",
      "step:    19  f: -2.868114902300  gradNorm: 2.473517488\n",
      "step:    20  f: -2.898007606623  gradNorm: 2.468251852\n",
      "step:    30  f: -3.764218259421  gradNorm: 2.152367948\n",
      "step:    40  f: -5.801929718846  gradNorm: 0.618026606\n",
      "step:    50  f: -6.888366145566  gradNorm: 0.142337890\n",
      "step:    60  f: -7.359779378381  gradNorm: 0.061575439\n",
      "step:    70  f: -7.458440084328  gradNorm: 0.033225607\n",
      "step:    80  f: -7.470619523126  gradNorm: 0.013383252\n",
      "step:    90  f: -7.473301046366  gradNorm: 0.006313361\n",
      "step:    99  f: -7.474435129203  gradNorm: 0.003533653\n",
      " took 130.7915 seconds \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for i in range(1):\n",
    "    print(\"Optimization restart: {}\".format(i))\n",
    "    xrestart = opt_energyrc(steps=100,num_basis=512, restart=False)\n",
    "print(\" took {:.4f} seconds \".format(time.time() - start_time))\n",
    "#cProfile.run('xprof = opt_energyrc()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#th.save(xrestart, 'Libo-nb1280p-7.478059.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xrestart = th.load('LiBO-wf-pts/Libo-CPU-nb512-7.478023.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = th.tensor(xrestart.cpu().detach().numpy(), device=device, dtype=dtype, requires_grad=True)\n",
    "#xrestart = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xrestart "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
