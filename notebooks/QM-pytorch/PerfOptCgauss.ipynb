{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Correlsted Gaussian Wave function \n",
    "\n",
    "Performance optimizations testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize a wave function expanded in correlated gaussian basis functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integral formulas  (Matrix Elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skl (Overlap), Tkl (Kinetic Energy), Vkl (Potential Energy) Hkl (Hamiltonion)\n",
    "\n",
    " **py_matel**: returnes symmetry projected matrix elements in a\n",
    " basis of simple correlated gaussians phi_kl = exp[-r'(Lk*Ll' kron I3)r]\n",
    "\n",
    " * n:      the number of \"psuedo\" particles i.e. Center of mass translationl degrees of freedom are removed so n is N-1 if N is the number of \"real\" particles.\n",
    "            \n",
    " * vechLk: nonlinear exponent parameters n(n+1)/2 x 1\n",
    " * vechLl:    These will form the lower triangle matrices Lk and Ll\n",
    "\n",
    "\n",
    " * Sym:     symmetry projection matrix for the term being computed\n",
    " * Mass:    mass matrix for kinetic energy (reduced masses of particles )\n",
    " * vecQ:    charge products for potential energy (elements q_i x q_j where q are particle charges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "import time\n",
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution device:  cpu\n",
      "PyTorch version:  0.4.1\n",
      "CUDA available:  True\n",
      "CUDA version:  9.0.176\n",
      "CUDA device: TITAN V\n"
     ]
    }
   ],
   "source": [
    "dtype = th.float64\n",
    "\n",
    "gpuid = 0\n",
    "#device = th.device(\"cuda:\"+ str(gpuid))\n",
    "device = th.device(\"cpu\")\n",
    "\n",
    "print(\"Execution device: \",device)\n",
    "print(\"PyTorch version: \", th.__version__ )\n",
    "print(\"CUDA available: \", th.cuda.is_available())\n",
    "print(\"CUDA version: \", th.version.cuda)\n",
    "print(\"CUDA device:\", th.cuda.get_device_name(gpuid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "# return the lower triangle of A in column order i.e. vech(A)\n",
    "def vech(A):\n",
    "    count = 0\n",
    "    c = A.shape[0]\n",
    "    v = th.zeros(c * (c + 1) // 2,)\n",
    "    for j in range(c):\n",
    "        for i in range(j,c):\n",
    "            v[count] = A[i,j]\n",
    "            count += 1\n",
    "    return th.tensor(v , device=device, dtype=dtype)\n",
    "\n",
    "# vech2L   create lower triangular matrix L from vechA\n",
    "def vech2L(v,n):\n",
    "    count = 0\n",
    "    L = th.zeros((n,n))\n",
    "    for j in range(n):\n",
    "        for i in range(j,n):\n",
    "            L[i,j]=v[count]\n",
    "            count += 1\n",
    "    return th.tensor(L , device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def py_matel(n, Ak, Al, detLk,detLl, Sym, Mass, Qmat):\n",
    "    \n",
    "\n",
    "    # build Lk and Ll\n",
    "\n",
    "    #Lk = vech2L(vechLk,n);\n",
    "    #Ll = vech2L(vechLl,n);\n",
    "    \n",
    "    # apply symmetry projection on Ll\n",
    "    \n",
    "    # th.t() is shorthand for th.transpose(X, 0,1)\n",
    "    #PLl = th.t(Sym) @ Ll;\n",
    "    \n",
    "    # build Ak, Al, Akl, invAkl, invAk, invAl\n",
    "\n",
    "    #Ak = Lk@th.t(Lk);\n",
    "    #Al = PLl@th.t(PLl);\n",
    "    Akl = Ak+Al;\n",
    "    \n",
    "    invAkl = th.inverse(Akl);\n",
    "    #invAk  = th.inverse(Ak);\n",
    "    #invAl  = th.inverse(Al);\n",
    "    \n",
    "    # Overlap: (normalized)\n",
    "    skl = 2**(n*1.5) * th.sqrt( th.pow(detLk*detLl/th.det(Akl) ,3) );\n",
    "\n",
    "    # kinetic energy\n",
    "\n",
    "    #tkl = skl*(6*th.trace(Mass@Ak@invAkl@Al));    \n",
    "    tkl = skl*(6*th.sum(Mass*(Ak@invAkl@Al)))\n",
    "    \n",
    "    # potential energy\n",
    "    \n",
    "    TWOoSqrtPI = 1.1283791670955126 # 2/sqrt(pi)\n",
    "    \n",
    "    RIJ = th.zeros((n,n), device=device, dtype=dtype);\n",
    "    # 1/rij i~=j\n",
    "    for j in range(0,n-1):\n",
    "        for i in range(j+1,n):\n",
    "            tmp2 = invAkl[i,i] + invAkl[j,j] - 2*invAkl[i,j];\n",
    "            #RIJ[i,j] = TWOoSqrtPI * skl/th.sqrt(tmp2);\n",
    "            RIJ[i,j] = th.rsqrt(tmp2)\n",
    "\n",
    "\n",
    "    # 1/rij i=j\n",
    "    for i in range(0,n):\n",
    "        #RIJ[i,i] = TWOoSqrtPI * skl/th.sqrt(invAkl[i,i]);\n",
    "        RIJ[i,i] = th.rsqrt(invAkl[i,i])\n",
    "    \n",
    "    RIJ = TWOoSqrtPI*skl*RIJ\n",
    "    \n",
    "    #Q = vech2L(vecQ,n);\n",
    "\n",
    "    vkl = th.sum(RIJ*Qmat)\n",
    "\n",
    "    #hkl = tkl + vkl\n",
    "    \n",
    "    # Gradient Terms\n",
    "    \n",
    "    #gradient with respect to vechLk vechLl\n",
    "    #checkdsk = vech( 3/2 * skl * (th.diag(1/th.diag(Lk)) - 2*invAkl@Lk) )\n",
    "    #checkdsl = vech( 3/2 * skl * (th.diag(1/th.diag(Ll)) - 2*Sym@invAkl@PLl) )\n",
    "    \n",
    "    #dsk = th.autograd.grad(skl, vechLk, retain_graph=True)  \n",
    "    #dsl = th.autograd.grad(skl, vechLl, retain_graph=True)\n",
    "    \n",
    "    #dtk = th.autograd.grad(tkl, vechLk, retain_graph=True)\n",
    "    #dtl = th.autograd.grad(tkl, vechLl, retain_graph=True)\n",
    "    \n",
    "    #dvk = th.autograd.grad(vkl, vechLk, retain_graph=True)\n",
    "    #dvl = th.autograd.grad(vkl, vechLl, retain_graph=True)\n",
    "    \n",
    "    #dhk = dtk[0] + dvk[0]\n",
    "    #dhl = dtl[0] + dvl[0]\n",
    "    \n",
    "    #chkdhk = th.autograd.grad(hkl, vechLk, retain_graph=True)\n",
    "    #chkdhl = th.autograd.grad(hkl, vechLl)\n",
    "    \n",
    "    return {'skl':skl, 'tkl':tkl, 'vkl':vkl#, 'hkl':hkl,\n",
    "            #'dsk':dsk, 'dsl':dsl, \n",
    "            #'checkdsk':checkdsk, 'checkdsl':checkdsl,\n",
    "            #'dtk':dtk, 'dtl':dtl, 'dvk':dvk, 'dvl':dvl,\n",
    "            #'dhk':dhk, \"dhl\":dhl,\n",
    "            #'chkdhk':chkdhk, \"chkdhl\":chkdhl\n",
    "           }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_matel():\n",
    "    n = 3;\n",
    "    vechLk = th.tensor([  1.00000039208682, \n",
    "              0.02548044275764261, \n",
    "              0.3525161612610669,\n",
    "              1.6669144815242515,\n",
    "              0.9630555318946559,\n",
    "              1.8382882034659822 ], device=device, dtype=dtype, requires_grad=True);\n",
    "    \n",
    "    vechLl = th.tensor([  1.3353550436464964,\n",
    "               0.9153272033682132,\n",
    "               0.7958636766525028,\n",
    "               1.8326931436447955,\n",
    "               0.3450426931160630,\n",
    "               1.8711839323167831 ], device=device, dtype=dtype, requires_grad=True);\n",
    "    \n",
    "    Sym = th.tensor([[0,0,1],\n",
    "                    [0,1,0],\n",
    "                    [1,0,0]], device=device, dtype=dtype);\n",
    "    \n",
    "    Mass = th.tensor([[5.446170e-4, 2.723085077e-4, 2.723085077e-4],\n",
    "                     [2.723085077e-4, .5002723085, 2.723085077e-4],\n",
    "                     [2.723085077e-4, 2.723085077e-4, .5002723085 ]], device=device, dtype=dtype);\n",
    "    \n",
    "    vecQ = th.tensor([1, -1, -1, -1, 1, -1], device=device, dtype=dtype);\n",
    "    \n",
    "    matels = py_matel(n, vechLk, vechLl, Sym, Mass, vecQ)\n",
    "    \n",
    "    print('skl: ',matels['skl'])\n",
    "    print('tkl: ',matels['tkl'])\n",
    "    print('vkl: ',matels['vkl'])\n",
    "    #print('hkl: ',matels['hkl'])\n",
    "    #print('dsk: ',matels['dsk'])\n",
    "    #print('dsl: ',matels['dsl'])\n",
    "    #print('checkdsk: ',matels['checkdsk'])\n",
    "    #print('checkdsl: ',matels['checkdsl'])\n",
    "    #print('dhk: ',matels['dhk'])\n",
    "    #print('dhl: ',matels['dhl'])\n",
    "    #print('dtk: ',matels['dtk'])\n",
    "    #print('dtl: ',matels['dtl'])\n",
    "    #print('dvk: ',matels['dvk'])\n",
    "    #print('dvl: ',matels['dvl'])\n",
    "    #print('dhk: ',matels['dhk'])\n",
    "    #print('dhl: ',matels['dhl'])\n",
    "    #print('chkdhk: ',matels['chkdhk'])\n",
    "    #print('chkdhl: ',matels['chkdhl'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skl:  tensor(0.5334, dtype=torch.float64, grad_fn=<MulBackward>)\n",
      "tkl:  tensor(4.3509, dtype=torch.float64, grad_fn=<ThMulBackward>)\n",
      "vkl:  tensor(-2.3840, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      " took 0.009138822555541992 seconds \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "test_matel()\n",
    "print(\" took {} seconds \".format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **energyrc**: returns the energy Rayleigh quotient c'Hc/c'Sc\n",
    " in a basis of simple correlated Gaussians. The minimum of hte Rayleigh quotent \n",
    " is the minimum of the smallest eigenvalue of the matrix representation of the Schrodinger euqation,\n",
    " (H-eS)c = 0 It is simpiler to compute than the full set of eigenvalues and verctors. A good optimization will determine the linear coeffients c i.e the eigenvector of the enery e.\n",
    "\n",
    "* x:\t\tthe optimization parameters\n",
    "* n:\t\tthe number of pseudo particles (size of Lk)\n",
    "* nb:\t\tthe number of basis functions\n",
    "\n",
    "* the first n(n+1)/2 * nb elements of x are exponent parameters (vechLk)\n",
    "* the last nb elements of x are the linear coeff's c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def py_energyrc(x,n,nb,Mass,Charge,Sym,symc):\n",
    "    \n",
    "    nx = len(x);\n",
    "    nn = int(n*(n+1)/2);\n",
    "    nsym = len(symc);\n",
    "    \n",
    "    # extract linear coefs \"eigen vector\"\n",
    "    c = x[-nb:];\n",
    "    # reshape non-linear variables for easier indexing\n",
    "    X = th.reshape(x[:nb*nn], (nb,nn))\n",
    "    #npX = X.detach().numpy()\n",
    "    L = th.zeros((nb,n,n), device=device, dtype=dtype)\n",
    "    for i in range(nb):\n",
    "        #L[i][np.tril_indices(n)]=npX[i,:]\n",
    "        L[i][:,:] = vech2L(X[i,:],n)\n",
    "    #L = th.from_numpy(L)\n",
    "    detL = th.abs(th.prod(th.diagonal(L, offset=0, dim1=-1, dim2=-2),1))\n",
    "    A = th.matmul(L,th.transpose(L, 1, 2))\n",
    "    \n",
    "    # Build H and S\n",
    "    H = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    S = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    T = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    V = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    #dS = th.zeros((nb,nb, nn), device=device, dtype=dtype);\n",
    "    #dT = th.zeros((nb,nb, nn), device=device, dtype=dtype);\n",
    "    #dV = th.zeros((nb,nb, nn), device=device, dtype=dtype);\n",
    "    \n",
    "    # outer loop is over symmetry terms\n",
    "    for k in range(0,nsym):\n",
    "        P = Sym[:,:,k]\n",
    "        PA = th.matmul(th.matmul(P,th.transpose(A, 1, 2)), th.t(P))\n",
    "        for j in range(0,nb):\n",
    "            for i in range(j,nb):\n",
    "                #idxi = i*nn;\n",
    "                #idxj = j*nn;\n",
    "                \n",
    "                #vechLi = x[idxi:idxi+nn];\n",
    "                #vechLj = x[idxj:idxj+nn];\n",
    "                Ai = A[i,:,:]\n",
    "                Aj = PA[j,:,:]\n",
    "                detLi = detL[i]\n",
    "                detLj = detL[j]\n",
    "                \n",
    "                matels = py_matel(n,Ai,Aj,detLi,detLj,Sym[:,:,k],Mass,Charge);\n",
    "                \n",
    "                S[i,j] += symc[k]*matels['skl'];\n",
    "                T[i,j] += symc[k]*matels['tkl'];\n",
    "                V[i,j] += symc[k]*matels['vkl'];\n",
    "                #dS[i,j,:] += symc[k]*matels['dsk'][0]\n",
    "                #dS[j,i,:] += symc[k]*matels['dsl'][0]\n",
    "                #dT[i,j,:] += symc[k]*matels['dtk'][0]\n",
    "                #dT[j,i,:] += symc[k]*matels['dtl'][0]\n",
    "                #dV[i,j,:] += symc[k]*matels['dvk'][0]\n",
    "                #dV[j,i,:] += symc[k]*matels['dvl'][0]\n",
    "                \n",
    "    H = T + V\n",
    "    #dH = dT + dV\n",
    "    \n",
    "    # complete upper triangle of H and S\n",
    "    for i in range(0,nb):\n",
    "        for j in range(i+1,nb):\n",
    "            H[i,j] = H[j,i];\n",
    "            S[i,j] = S[j,i];\n",
    "            \n",
    "    # and the energy is:\n",
    "    #c = x[-nb:];\n",
    "    #cHc = th.t(c)@H@c;\n",
    "    #cSc = th.t(c)@S@c;\n",
    "    cHc = c@H@c;\n",
    "    cSc = c@S@c;\n",
    "    eng = cHc/cSc;\n",
    "    #cc = th.ger(c,c)\n",
    "    #C = 2*cc - th.diag(th.diag(cc))\n",
    "    #G = dH - eng*dS\n",
    "    #for i in range(nn):\n",
    "    #    G[:,:,i] *= C\n",
    "    #G = th.sum(G,0).view(-1)\n",
    "    #return (eng, G)\n",
    "    #print(th.autograd.grad(eng, x , retain_graph=True))\n",
    "    return eng           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_energyrc():\n",
    "        \n",
    "    n=3;\n",
    "    nb=8;\n",
    "    \n",
    "    Mass = th.tensor([[0.5, 0.0, 0.0],\n",
    "                     [0.0, 0.5, 0.0],\n",
    "                     [0.0, 0.0, 0.5]], device=device, dtype=dtype);\n",
    "    \n",
    "    Charge = th.tensor([-3, 1, 1, -3, 1, -3], device=device, dtype=dtype);\n",
    "    Charge = vech2L(Charge,n)\n",
    "    \n",
    "    # symmetry projection terms\n",
    "    Sym = th.zeros((3,3,6), device=device, dtype=dtype)\n",
    "    # (1)(2)(3)\n",
    "    Sym[:,:,0] = th.tensor([[1,0,0],[0,1,0],[0,0,1]], device=device, dtype=dtype);\n",
    "    # (12)\n",
    "    Sym[:,:,1] = th.tensor([[0,1,0],[1,0,0],[0,0,1]], device=device, dtype=dtype);\n",
    "    # (13)\n",
    "    Sym[:,:,2] = th.tensor([[0,0,1],[0,1,0],[1,0,0]], device=device, dtype=dtype);\n",
    "    # (23)\n",
    "    Sym[:,:,3] = th.tensor([[1,0,0],[0,0,1],[0,1,0]], device=device, dtype=dtype);\n",
    "    # (123)\n",
    "    Sym[:,:,4] = th.tensor([[0,1,0],[0,0,1],[1,0,0]], device=device, dtype=dtype);\n",
    "    # (132)\n",
    "    Sym[:,:,5] = th.tensor([[0,0,1],[1,0,0],[0,1,0]], device=device, dtype=dtype);\n",
    "\n",
    "    # coeff's\n",
    "    symc = th.tensor([4.0,4.0,-2.0,-2.0,-2.0,-2.0], device=device, dtype=dtype);\n",
    "\n",
    "    \n",
    "    xvechL=th.tensor([\n",
    "     1.6210e+00,\n",
    "    -2.1504e-01,\n",
    "     9.0755e-01,\n",
    "     9.7866e-01,\n",
    "    -2.8418e-01,\n",
    "    -3.5286e+00,\n",
    "    -3.3045e+00,\n",
    "    -4.5036e+00,\n",
    "    -3.2116e-01,\n",
    "    -7.1901e-02,\n",
    "     1.5167e+00,\n",
    "    -8.4489e-01,\n",
    "    -2.1377e-01,\n",
    "    -3.6127e-03,\n",
    "    -5.3774e-03,\n",
    "    -2.1263e+00,\n",
    "    -2.5191e-01,\n",
    "     2.1235e+00,\n",
    "    -2.1396e-01,\n",
    "    -1.4084e-03,\n",
    "    -1.0092e-02,\n",
    "     4.5349e+00,\n",
    "     9.4837e-03,\n",
    "     1.1225e+00,\n",
    "    -2.1315e-01,\n",
    "     5.8451e-02,\n",
    "    -4.9410e-03,\n",
    "     5.0853e+00,\n",
    "     7.3332e-01,\n",
    "     5.0672e+00,\n",
    "    -2.1589e-01,\n",
    "    -6.8986e-03,\n",
    "    -1.4310e-02,\n",
    "     1.5979e+00,\n",
    "     3.3946e-02,\n",
    "    -8.7965e-01,\n",
    "    -1.1121e+00,\n",
    "    -2.1903e-03,\n",
    "    -4.6925e-02,\n",
    "     2.1457e-01,\n",
    "     3.3045e-03,\n",
    "     4.5120e+00,\n",
    "    -2.1423e-01,\n",
    "    -1.6493e-02,\n",
    "    -2.3429e-03,\n",
    "    -8.6715e-01,\n",
    "    -6.7070e-02,\n",
    "     1.5998e+00\n",
    "     ], device=device, dtype=dtype, requires_grad=False)\n",
    "    \n",
    "    evec = th.tensor([\n",
    "      -6.0460e-02,\n",
    "       7.7708e-05,\n",
    "       1.6152e+00,\n",
    "       9.5443e-01,\n",
    "       1.1771e-01,\n",
    "       3.2196e+00,\n",
    "       9.6344e-01,\n",
    "       3.1398e+00\n",
    "    ], device=device, dtype=dtype, requires_grad=False)\n",
    "    \n",
    "    #x1 = th.tensor(th.cat((xvechL,evec)), device=device, dtype=dtype, requires_grad=True)\n",
    "    n=3;\n",
    "    nb=96;\n",
    "    th.manual_seed(42)\n",
    "    #x1 = th.randn(int(nb*n*(n+1)/2 + nb) , device=device, dtype=dtype, requires_grad=True)\n",
    "    x1 = xrestart\n",
    "    #print(x1)\n",
    "    #energy, G = py_energyrc(x1,n,nb,Mass,Charge,Sym,symc)\n",
    "    #energy = py_energyrc(x1,n,nb,Mass,Charge,Sym,symc)\n",
    "    #print(energy)\n",
    "    #print(th.autograd.grad(energy, x1))\n",
    "    \n",
    "    #optimizer = th.optim.LBFGS([x1])\n",
    "    optimizer = th.optim.Adadelta([x1], lr=0.5)\n",
    "    #optimizer = th.optim.Adam([x1], lr=0.1)\n",
    "    \n",
    "    #scheduler = th.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True,patience=2)\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer.zero_grad()\n",
    "        loss = py_energyrc(x1,n,nb,Mass,Charge,Sym,symc)\n",
    "        loss.backward()\n",
    "        #def closure():\n",
    "        #    return py_energyrc(x1,n,nb,Mass,Charge,Sym,symc)\n",
    "        optimizer.step()\n",
    "        #scheduler.step(loss)\n",
    "        \n",
    "        print('step: {} f: {} gradNorm: {}'.format(i, loss, th.norm(x1.grad)))\n",
    "    \n",
    "    return x1\n",
    "    #from scipy import optimize\n",
    "    #eps = th.tensor(1e-08)\n",
    "    #optimize.approx_fprime(x1, py_energyrc, eps, n,nb,Mass,Charge,Sym,symc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 f: -7.460833513991575 gradNorm: 0.17131797946997435\n",
      "step: 1 f: -7.4619883653930215 gradNorm: 0.11479722281867605\n",
      " took 52.56346678733826 seconds \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback BaseAsyncIOLoop._handle_events(11, 1)\n",
      "handle: <Handle BaseAsyncIOLoop._handle_events(11, 1)>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kinghorn/anaconda3/envs/pytorch/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/home/kinghorn/anaconda3/envs/pytorch/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/home/kinghorn/anaconda3/envs/pytorch/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/kinghorn/anaconda3/envs/pytorch/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/kinghorn/anaconda3/envs/pytorch/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/kinghorn/anaconda3/envs/pytorch/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/kinghorn/anaconda3/envs/pytorch/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/kinghorn/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/kinghorn/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 204, in dispatch_shell\n",
      "    idents,msg = self.session.feed_identities(msg, copy=False)\n",
      "  File \"/home/kinghorn/anaconda3/envs/pytorch/lib/python3.6/site-packages/jupyter_client/session.py\", line 853, in feed_identities\n",
      "    raise ValueError(\"DELIM not in msg_list\")\n",
      "ValueError: DELIM not in msg_list\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "xrestart = test_energyrc()\n",
    "print(\" took {} seconds \".format(time.time() - start_time))\n",
    "#cProfile.run('xrestart = test_energyrc()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing 2 optimization steps  initial time 42 sec\n",
    "\n",
    "Changing Charge to a matrix removed 25000 ops and took time to 31.4 sec\n",
    "\n",
    "converted vevhL to L before matel time now 22.8 sec\n",
    "\n",
    "convert trace to sum for tkl, 1/2 to 0.5, remove 1 abs()  -> 21.9 sec\n",
    "\n",
    "batched abs(det(Lk))  -> 21.5 sec\n",
    "\n",
    "batched Ak, PAlP' -> 19.8 sec\n",
    "\n",
    "th.rsqrt(invAkl) -> 21.3 (timeing has changed with new restart pt!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.save(xrestart, 'Libo-nb96-7.460.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrestart = th.load('Libo-nb96-7.457.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
