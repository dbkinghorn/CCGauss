{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th # PyTorch is imported as torch and will be referenced as \"th\"\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution device:  cpu\n",
      "PyTorch version:  0.4.1\n",
      "CUDA available:  True\n",
      "CUDA version:  9.0.176\n",
      "CUDA device: TITAN V\n"
     ]
    }
   ],
   "source": [
    "dtype = th.float64\n",
    "\n",
    "gpuid = 0\n",
    "#device = th.device(\"cuda:\"+ str(gpuid))\n",
    "device = th.device(\"cpu\")\n",
    "\n",
    "print(\"Execution device: \",device)\n",
    "print(\"PyTorch version: \", th.__version__ )\n",
    "print(\"CUDA available: \", th.cuda.is_available())\n",
    "print(\"CUDA version: \", th.version.cuda)\n",
    "print(\"CUDA device:\", th.cuda.get_device_name(gpuid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for working with lower triangular matrices\n",
    "\n",
    "# return the lower triangle of A in column order i.e. vech(A)\n",
    "def vech(A):\n",
    "    count = 0\n",
    "    c = A.shape[0]\n",
    "    v = th.zeros(c * (c + 1) // 2,)\n",
    "    for j in range(c):\n",
    "        for i in range(j,c):\n",
    "            v[count] = A[i,j]\n",
    "            count += 1\n",
    "    return th.tensor(v , device=device, dtype=dtype)\n",
    "\n",
    "# vech2L   create lower triangular matrix L from vechA\n",
    "def vech2L(v,n):\n",
    "    count = 0\n",
    "    L = th.zeros((n,n))\n",
    "    for j in range(n):\n",
    "        for i in range(j,n):\n",
    "            L[i,j]=v[count]\n",
    "            count += 1\n",
    "    return th.tensor(L , device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_elements(n, vechLk, vechLl, Sym, Mass, vecQ):\n",
    "    '''\n",
    "    Returns: a dictionary with the skl, tkl, vkl matrix elements\n",
    "    n : the size of the Lk matrices\n",
    "    vechL(k,l): vector of parameters use for Lk and Ll matrices\n",
    "    Sym: a single symmetry projection matrix\n",
    "    Mass: a matrix of \"reduced mass\" values for the particles needed for tkl\n",
    "    vecQ: an array of products of particle charges needed for vkl   \n",
    "    '''\n",
    "    # build Lk and Ll\n",
    "\n",
    "    Lk = vech2L(vechLk,n);\n",
    "    Ll = vech2L(vechLl,n);\n",
    "\n",
    "    # apply symmetry projection on Ll\n",
    "\n",
    "    # th.t() is shorthand for th.transpose(X, 0,1)\n",
    "    PLl = th.t(Sym) @ Ll;\n",
    "\n",
    "    # build Ak, Al, Akl, invAkl\n",
    "\n",
    "    Ak = Lk@th.t(Lk);\n",
    "    Al = PLl@th.t(PLl);\n",
    "    Akl = Ak+Al\n",
    "    invAkl = th.inverse(Akl);\n",
    "\n",
    "    # Overlap (normalized)\n",
    "    skl = 2**(3*n/2) * th.sqrt( th.pow(th.abs(th.det(Lk))*th.abs(th.det(Ll))/th.det(Akl) ,3) );\n",
    "\n",
    "    # Kinetic energy\n",
    "\n",
    "    tkl = skl*(6*th.trace(Mass@Ak@invAkl@Al));\n",
    "\n",
    "    # potential energy\n",
    "\n",
    "    TWOoSqrtPI = 1.1283791670955126 # 2/sqrt(pi)\n",
    "\n",
    "    RIJ = th.zeros((n,n), device=device, dtype=dtype);\n",
    "    # 1/rij i~=j\n",
    "    for j in range(0,n-1):\n",
    "        for i in range(j+1,n):\n",
    "            tmp2 = invAkl[i,i] + invAkl[j,j] - 2*invAkl[i,j];\n",
    "            #RIJ[i,j] = TWOoSqrtPI * skl/th.sqrt(tmp2);\n",
    "            RIJ[i,j] = 1/th.sqrt(tmp2)\n",
    "\n",
    "    # 1/rij i=j\n",
    "    for i in range(0,n):\n",
    "        #RIJ[i,i] = TWOoSqrtPI * skl/th.sqrt(invAkl[i,i]);\n",
    "        RIJ[i,i] = 1/th.sqrt(invAkl[i,i])\n",
    "\n",
    "    RIJ = TWOoSqrtPI*skl*RIJ    \n",
    "    Q = vech2L(vecQ,n);\n",
    "\n",
    "    vkl = th.sum(RIJ*Q)\n",
    "\n",
    "    return {'skl':skl, 'tkl':tkl, 'vkl':vkl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_matrix_elements():\n",
    "# test input with a known result\n",
    "    n = 3;\n",
    "    vechLk = th.tensor([  1.00000039208682,\n",
    "              0.02548044275764261,\n",
    "              0.3525161612610669,\n",
    "              1.6669144815242515,\n",
    "              0.9630555318946559,\n",
    "              1.8382882034659822 ], device=device, dtype=dtype, requires_grad=True);\n",
    "\n",
    "    vechLl = th.tensor([  1.3353550436464964,\n",
    "               0.9153272033682132,\n",
    "               0.7958636766525028,\n",
    "               1.8326931436447955,\n",
    "               0.3450426931160630,\n",
    "               1.8711839323167831 ], device=device, dtype=dtype, requires_grad=True);\n",
    "\n",
    "    Sym = th.tensor([[0,0,1],\n",
    "                    [0,1,0],\n",
    "                    [1,0,0]], device=device, dtype=dtype);\n",
    "\n",
    "    Mass = th.tensor([[5.446170e-4, 2.723085077e-4, 2.723085077e-4],\n",
    "                     [2.723085077e-4, .5002723085, 2.723085077e-4],\n",
    "                     [2.723085077e-4, 2.723085077e-4, .5002723085 ]], device=device, dtype=dtype);\n",
    "\n",
    "    vecQ = th.tensor([1, -1, -1, -1, 1, -1], device=device, dtype=dtype);\n",
    "\n",
    "    matels = matrix_elements(n, vechLk, vechLl, Sym, Mass, vecQ)\n",
    "\n",
    "    print('skl: ',matels['skl']) # should be 0.5334\n",
    "    print('tkl: ',matels['tkl']) # should be 4.3509\n",
    "    print('vkl: ',matels['vkl']) # should be -2.3840"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skl:  tensor(0.5334, dtype=torch.float64, grad_fn=<MulBackward>)\n",
      "tkl:  tensor(4.3509, dtype=torch.float64, grad_fn=<ThMulBackward>)\n",
      "vkl:  tensor(-2.3840, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      " took 0.035093 seconds \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "test_matrix_elements()\n",
    "print(\" took {:.6f} seconds \".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy(x,n,nb,Mass,Charge,Sym,symc):\n",
    "    '''\n",
    "    Returns: the energy at the point (parameters) x\n",
    "    n: number of particles defined in input\n",
    "    nb: number of basis functions\n",
    "    Mass: matrix of \"reduced\" mass constants for system\n",
    "    Charge: vector of \"charge products\" for particle pairs\n",
    "    Sym: a tensor containing the symmetry projection matrices\n",
    "    symc: a vector of coefficients for the symmetry projection terms\n",
    "    '''\n",
    "\n",
    "    nx = len(x);\n",
    "    nn = int(n*(n+1)/2);\n",
    "    nsym = len(symc);\n",
    "\n",
    "    # extract linear coefs \"eigen vector\" from x\n",
    "    c = x[-nb:];\n",
    "    # reshape non-linear variables for easier indexing\n",
    "    X = th.reshape(x[:nb*nn], (nb,nn))\n",
    "\n",
    "    # Init H S T V\n",
    "    H = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    S = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    T = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    V = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "\n",
    "    # outer loop is over symmetry terms\n",
    "    for k in range(0,nsym):\n",
    "\n",
    "        for j in range(0,nb):\n",
    "            for i in range(j,nb):\n",
    "\n",
    "                vechLi = X[i,:]\n",
    "                vechLj = X[j,:]\n",
    "\n",
    "                matels = matrix_elements(n,vechLi,vechLj,Sym[:,:,k],Mass,Charge);\n",
    "\n",
    "                S[i,j] += symc[k]*matels['skl'];\n",
    "                T[i,j] += symc[k]*matels['tkl'];\n",
    "                V[i,j] += symc[k]*matels['vkl'];\n",
    "\n",
    "    H = T + V\n",
    "\n",
    "    # complete upper triangle of H and S\n",
    "    for i in range(0,nb):\n",
    "        for j in range(i+1,nb):\n",
    "            H[i,j] = H[j,i];\n",
    "            S[i,j] = S[j,i];\n",
    "\n",
    "    # The energy from the Rayleigh quotient\n",
    "\n",
    "    cHc = c@H@c;\n",
    "    cSc = c@S@c;\n",
    "    eng = cHc/cSc;\n",
    "\n",
    "    return eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_energy():\n",
    "\n",
    "    Mass = th.tensor([[0.5, 0.0, 0.0],\n",
    "                     [0.0, 0.5, 0.0],\n",
    "                     [0.0, 0.0, 0.5]], device=device, dtype=dtype);\n",
    "\n",
    "    Charge = th.tensor([-3, 1, 1, -3, 1, -3], device=device, dtype=dtype);\n",
    "\n",
    "    # symmetry projection terms\n",
    "    Sym = th.zeros((3,3,6), device=device, dtype=dtype)\n",
    "    # (1)(2)(3)\n",
    "    Sym[:,:,0] = th.tensor([[1,0,0],[0,1,0],[0,0,1]], device=device, dtype=dtype);\n",
    "    # (12)\n",
    "    Sym[:,:,1] = th.tensor([[0,1,0],[1,0,0],[0,0,1]], device=device, dtype=dtype);\n",
    "    # (13)\n",
    "    Sym[:,:,2] = th.tensor([[0,0,1],[0,1,0],[1,0,0]], device=device, dtype=dtype);\n",
    "    # (23)\n",
    "    Sym[:,:,3] = th.tensor([[1,0,0],[0,0,1],[0,1,0]], device=device, dtype=dtype);\n",
    "    # (123)\n",
    "    Sym[:,:,4] = th.tensor([[0,1,0],[0,0,1],[1,0,0]], device=device, dtype=dtype);\n",
    "    # (132)\n",
    "    Sym[:,:,5] = th.tensor([[0,0,1],[1,0,0],[0,1,0]], device=device, dtype=dtype);\n",
    "\n",
    "    # coeff's\n",
    "    symc = th.tensor([4.0,4.0,-2.0,-2.0,-2.0,-2.0], device=device, dtype=dtype);\n",
    "\n",
    "    n=3;\n",
    "    nb=8;\n",
    "\n",
    "    xvechL=th.tensor([\n",
    "         1.6210e+00, -2.1504e-01,  9.0755e-01,  9.7866e-01, -2.8418e-01,\n",
    "        -3.5286e+00, -3.3045e+00, -4.5036e+00, -3.2116e-01, -7.1901e-02,\n",
    "         1.5167e+00, -8.4489e-01, -2.1377e-01, -3.6127e-03, -5.3774e-03,\n",
    "        -2.1263e+00, -2.5191e-01,  2.1235e+00, -2.1396e-01, -1.4084e-03,\n",
    "        -1.0092e-02,  4.5349e+00,  9.4837e-03,  1.1225e+00, -2.1315e-01,\n",
    "         5.8451e-02, -4.9410e-03,  5.0853e+00,  7.3332e-01,  5.0672e+00,\n",
    "        -2.1589e-01, -6.8986e-03, -1.4310e-02,  1.5979e+00,  3.3946e-02,\n",
    "        -8.7965e-01, -1.1121e+00, -2.1903e-03, -4.6925e-02,  2.1457e-01,\n",
    "         3.3045e-03,  4.5120e+00, -2.1423e-01, -1.6493e-02, -2.3429e-03,\n",
    "        -8.6715e-01, -6.7070e-02,  1.5998e+00\n",
    "     ], device=device, dtype=dtype, requires_grad=False)\n",
    "\n",
    "    evec = th.tensor([\n",
    "      -6.0460e-02,  7.7708e-05, 1.6152e+00,  9.5443e-01,  \n",
    "      1.1771e-01,  3.2196e+00,  9.6344e-01, 3.1398e+00\n",
    "    ], device=device, dtype=dtype, requires_grad=False)\n",
    "\n",
    "    # join the parameters into x1\n",
    "    x1 = th.tensor(th.cat((xvechL,evec)), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "    eng = energy(x1,n,nb,Mass,Charge,Sym,symc)\n",
    "    print(eng) # Should return -7.3615 at this point x1\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-7.3615, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
      " took 0.11813998222351074 seconds \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "xrestart = test_energy()\n",
    "print(\" took {} seconds \".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_energy(steps=1, num_basis=8):\n",
    "\n",
    "    ############################################################################\n",
    "    # Input constants for Li atom (infinite nuclear mass)\n",
    "    ############################################################################\n",
    "    Mass = th.tensor([[0.5, 0.0, 0.0],\n",
    "                     [0.0, 0.5, 0.0],\n",
    "                     [0.0, 0.0, 0.5]], device=device, dtype=dtype);\n",
    "\n",
    "    Charge = th.tensor([-3, 1, 1, -3, 1, -3], device=device, dtype=dtype);\n",
    "\n",
    "    # symmetry projection terms\n",
    "    Sym = th.zeros((3,3,6), device=device, dtype=dtype)\n",
    "    # (1)(2)(3)\n",
    "    Sym[:,:,0] = th.tensor([[1,0,0],[0,1,0],[0,0,1]], device=device, dtype=dtype);\n",
    "    # (12)\n",
    "    Sym[:,:,1] = th.tensor([[0,1,0],[1,0,0],[0,0,1]], device=device, dtype=dtype);\n",
    "    # (13)\n",
    "    Sym[:,:,2] = th.tensor([[0,0,1],[0,1,0],[1,0,0]], device=device, dtype=dtype);\n",
    "    # (23)\n",
    "    Sym[:,:,3] = th.tensor([[1,0,0],[0,0,1],[0,1,0]], device=device, dtype=dtype);\n",
    "    # (123)\n",
    "    Sym[:,:,4] = th.tensor([[0,1,0],[0,0,1],[1,0,0]], device=device, dtype=dtype);\n",
    "    # (132)\n",
    "    Sym[:,:,5] = th.tensor([[0,0,1],[1,0,0],[0,1,0]], device=device, dtype=dtype);\n",
    "\n",
    "    # coeff's\n",
    "    symc = th.tensor([4.0,4.0,-2.0,-2.0,-2.0,-2.0], device=device, dtype=dtype);\n",
    "    ############################################################################\n",
    "\n",
    "\n",
    "    n=3\n",
    "    nb=num_basis\n",
    "    th.manual_seed(3)\n",
    "    x1 = th.empty(int(nb*n*(n+1)/2 + nb), device=device, dtype=dtype, requires_grad=True)\n",
    "    th.nn.init.uniform_(x1, a=-.8, b=.8)    \n",
    "\n",
    "    #x1 = xrestart\n",
    "\n",
    "    optimizer = th.optim.Rprop([x1], lr=0.001, etas=(0.5, 1.2), step_sizes=(1e-06, 50))\n",
    "\n",
    "    for i in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        loss = energy(x1,n,nb,Mass,Charge,Sym,symc)\n",
    "        loss.backward()      \n",
    "        optimizer.step()\n",
    "\n",
    "        if (i<10 or not i%10):print('step: {:5}  f: {:4.12f}  gradNorm: {:.9f}'.format(i, loss, th.norm(x1.grad)))\n",
    "    print('step: {:5}  f: {:4.12f}  gradNorm: {:.9f}'.format(i, loss, th.norm(x1.grad)))\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:     0  f: -0.782443089544  gradNorm: 6.657278345\n",
      "step:     1  f: -0.813405818204  gradNorm: 6.663333179\n",
      "step:     2  f: -0.850565814089  gradNorm: 6.664066434\n",
      "step:     3  f: -0.895095668330  gradNorm: 6.655362804\n",
      "step:     4  f: -0.948326026794  gradNorm: 6.631036613\n",
      "step:     5  f: -1.011714048225  gradNorm: 6.582179112\n",
      "step:     6  f: -1.086766887327  gradNorm: 6.496718392\n",
      "step:     7  f: -1.174915424078  gradNorm: 6.359865888\n",
      "step:     8  f: -1.277512892162  gradNorm: 6.162854834\n",
      "step:     9  f: -1.395322269810  gradNorm: 5.891109978\n",
      "step:    10  f: -1.528494104459  gradNorm: 5.568453572\n",
      "step:    20  f: -4.012585110663  gradNorm: 2.752546989\n",
      "step:    30  f: -6.571789033344  gradNorm: 1.543067756\n",
      "step:    40  f: -7.223244601602  gradNorm: 0.284565245\n",
      "step:    50  f: -7.289228643587  gradNorm: 0.149059481\n",
      "step:    60  f: -7.353758373450  gradNorm: 0.142130627\n",
      "step:    70  f: -7.368628179269  gradNorm: 0.069057239\n",
      "step:    80  f: -7.378787154917  gradNorm: 0.062784281\n",
      "step:    90  f: -7.389791432408  gradNorm: 0.047265743\n",
      "step:    99  f: -7.398805061984  gradNorm: 0.095278731\n",
      " took 28.317583560943604 seconds \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "xrestart = opt_energy(steps=100, num_basis=8)\n",
    "print(\" took {} seconds \".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:     0  f: -0.720436686340  gradNorm: 4.060212842\n",
      "step:     0  f: -0.720436686340  gradNorm: 4.060212842\n",
      " took 2768.9312148094177 seconds \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "xrestart = opt_energy(steps=1, num_basis=512)\n",
    "print(\" took {} seconds \".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Optimized Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def vech2L(v,n):\n",
    "    count = 0\n",
    "    L = th.zeros((n,n))\n",
    "    for j in range(n):\n",
    "        for i in range(j,n):\n",
    "            L[i,j]=v[count]\n",
    "            count = count + 1\n",
    "    return th.tensor(L , device=device, dtype=dtype)\n",
    "\n",
    "# batched vech2L input is \"X\" as V nb x n(n+1)/2\n",
    "def bvech2L(V,nb,n):\n",
    "    count = 0\n",
    "    L = th.zeros((nb,n,n))\n",
    "    for j in range(n):\n",
    "        for i in range(j,n):\n",
    "            L[...,i,j]=V[...,count]\n",
    "            count = count + 1\n",
    "    return th.tensor(L, device=device, dtype=dtype) + th.tensor(th.eye(n), device=device, dtype=dtype)\n",
    "\n",
    "# Batched Cholesky decomp\n",
    "def cholesky(A):\n",
    "    L = th.zeros_like(A)\n",
    "    \n",
    "    for i in range(A.shape[-1]):\n",
    "        for j in range(i+1):\n",
    "            s = 0.0\n",
    "            for k in range(j):\n",
    "                s = s + L[...,i,k].clone() * L[...,j,k].clone()\n",
    "            \n",
    "            L[...,i,j] = th.sqrt(A[...,i,i] - s) if (i == j) else \\\n",
    "                      (1.0 / L[...,j,j].clone() * (A[...,i,j] - s))\n",
    "    return L\n",
    "\n",
    "# Batched inverse of lower triangular matrices \n",
    "def inverseL(L):\n",
    "    n = L.shape[-1]\n",
    "    invL = th.zeros_like(L)\n",
    "    for j in range(0,n):\n",
    "        invL[...,j,j] = 1.0/L[...,j,j]\n",
    "        for i in range(j+1,n):\n",
    "            S = 0.0\n",
    "            for k in range(i+1):\n",
    "                S = S - L[...,i,k]*invL[...,k,j].clone()\n",
    "            invL[...,i,j] = S/L[...,i,i]\n",
    "\n",
    "    return invL\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_energyrc(x,n,nb,Mass,Qmat,Sym,symc):\n",
    "    \n",
    "    nx = len(x);\n",
    "    nn = int(n*(n+1)/2);\n",
    "    nsym = len(symc);\n",
    "    \n",
    "    # extract linear coefs \"eigen vector\"\n",
    "    c = x[-nb:];\n",
    "    # reshape non-linear variables for easier indexing\n",
    "    X = th.reshape(x[:nb*nn], (nb,nn))\n",
    "    \n",
    "    # generate tensor of lower triangular matrices from X\n",
    "    # these are the non-linear parameters of the basis set\n",
    "    L = th.zeros((nb,n,n), device=device, dtype=dtype)\n",
    "    L = bvech2L(X,nb,n)\n",
    "    \n",
    "    # get the determinates for L |L| is the product of diag elements\n",
    "    detL = th.abs(th.prod(th.diagonal(L, offset=0, dim1=-1, dim2=-2),1))\n",
    "    \n",
    "    # create the tensor of matrix products of the L matrices AKL = L x Ltranspose\n",
    "    AK = th.matmul(L,th.transpose(L, 1, 2))\n",
    "\n",
    "    \n",
    "    # Initialize H T V and S matrices\n",
    "    # H = T + V, we are solving (H-ES)c = 0 for E (energy)\n",
    "    H = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    S = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    T = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    V = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    \n",
    "\n",
    "    # outer loop is over symmetry terms, the matrices are summed over these sym terms\n",
    "    for k in range(0,nsym):\n",
    "        \n",
    "        P = Sym[k,:,:]\n",
    "        # symetry projection is applied only to \"ket\" this constructs AL\n",
    "        AL = th.matmul(th.t(P), th.matmul(AK,P))\n",
    "\n",
    "        # Akl = Ak + Al\n",
    "        AKL = th.zeros((nb,nb,n,n), device=device, dtype=dtype)\n",
    "        #for i in range(nb):\n",
    "        #    for j in range(nb):\n",
    "        #        #AKL[i,j] =  th.add(AK[i], AL[j])\n",
    "        #        AKL[i,j] =  AK[i] + AL[j]\n",
    "        AKL = AL.repeat((nb,1,1,1)) + th.transpose(AK.repeat((nb,1,1,1)), 0,1)\n",
    "        \n",
    "        # get the Cholesky decomp of all Akl martices\n",
    "        cholAKL = cholesky(AKL)\n",
    "        \n",
    "        # get determinates of AKL from diags |Akl|= |Lk|**2\n",
    "        detAKL = th.prod(th.diagonal(cholAKL, offset=0, dim1=-1, dim2=-2),-1)**2\n",
    "        \n",
    "        # compute inverses of lower tringular matrices in cholAKL\n",
    "        invLKL = inverseL(cholAKL)\n",
    "        \n",
    "        # inverses Akl^-1 = Lkl' x Lkl\n",
    "        invAKL = th.matmul(th.transpose(invLKL, dim0=-1, dim1=-2),invLKL)\n",
    "\n",
    "        # get terms needed for potential energy V\n",
    "        RIJ = th.zeros_like(invAKL, device=device, dtype=dtype);\n",
    "        # 1/rij i~=j\n",
    "        for j in range(0,n-1):\n",
    "            for i in range(j+1,n):\n",
    "                tmp2 = invAKL[...,i,i] + invAKL[...,j,j] - 2*invAKL[...,i,j];\n",
    "                RIJ[...,i,j] = th.rsqrt(tmp2)\n",
    "\n",
    "        # 1/rij i=j\n",
    "        for i in range(0,n):\n",
    "            RIJ[...,i,i] = th.rsqrt(invAKL[...,i,i])    \n",
    "\n",
    "        # MATRIX ELEMENTS\n",
    "        \n",
    "        # Overlap: (normalized)\n",
    "        # Skl = 2^3n/2 (||Lk|| ||Ll||/|AKL|)^3/2\n",
    "        SKL = 2**(n*1.5) * th.sqrt( th.pow(th.ger(detL, detL)/detAKL ,3) );\n",
    "\n",
    "        # Kinetic energy\n",
    "        #TKL = SKL*(6*th.trace(Mass@Ak@invAkl@Al)) = skl*(6*th.sum(Mass*(Ak@invAkl@Al)))\n",
    "\n",
    "        Tmat = th.zeros_like(invAKL)\n",
    "        #for i in range(nb):\n",
    "        #    for j in range(nb):\n",
    "        #        Tmat[i,j] = (AK[i]@invAKL[i,j]@AL[j])\n",
    "        Tmat = th.matmul(th.transpose(AK.repeat((nb,1,1,1)), 0,1), th.matmul(invAKL,AL))\n",
    "        TKL = 6*SKL*th.sum(Mass*Tmat, dim=(-2,-1))\n",
    "\n",
    "        # potential energy\n",
    "        TWOoSqrtPI = 1.1283791670955126 # 2/sqrt(pi)\n",
    "        \n",
    "        VKL = TWOoSqrtPI*SKL*th.sum(RIJ*Qmat, dim=(-2,-1))\n",
    "    \n",
    "        # accumulate matrices over sym terms\n",
    "        S = S + symc[k]*SKL\n",
    "        T = T + symc[k]*TKL\n",
    "        V = V + symc[k]*VKL\n",
    "        \n",
    "    # Hamiltonian\n",
    "    H = T + V\n",
    "    \n",
    "    # complete lower triangle of H and S\n",
    "    #for i in range(0,nb):\n",
    "    #    for j in range(i+1,nb):\n",
    "    #        H[j,i] = H[i,j]\n",
    "    #        S[j,i] = S[i,j]\n",
    "    #        #H[i,j] = H[j,i];\n",
    "    #        #S[i,j] = S[j,i];\n",
    "    H = th.triu(H,1)+th.t(th.triu(H))\n",
    "    S = th.triu(S,1)+th.t(th.triu(S))\n",
    "    # compute Rayleigh quotent (it is the smallest energy eigen value when minimized over c)\n",
    "    cHc = c@H@c;\n",
    "    cSc = c@S@c;\n",
    "    eng = cHc/cSc;\n",
    "    \n",
    "    return eng           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_energyrc(steps=1, num_basis=8, restart=True):\n",
    "    \n",
    "    #\n",
    "    # Li BO setup\n",
    "    #\n",
    "    n=3;\n",
    "    \n",
    "    Mass = th.tensor([[0.5, 0.0, 0.0],\n",
    "                     [0.0, 0.5, 0.0],\n",
    "                     [0.0, 0.0, 0.5]], device=device, dtype=dtype);\n",
    "    \n",
    "    Charge = th.tensor([-3, 1, 1, -3, 1, -3], device=device, dtype=dtype);\n",
    "    Charge = vech2L(Charge,n)\n",
    "    \n",
    "    # symmetry projection terms\n",
    "    Sym = th.zeros((6,3,3), device=device, dtype=dtype)\n",
    "    # (1)(2)(3)\n",
    "    Sym[0,:,:] = th.tensor([[1,0,0],[0,1,0],[0,0,1]], device=device, dtype=dtype);\n",
    "    # (12)\n",
    "    Sym[1,:,:] = th.tensor([[0,1,0],[1,0,0],[0,0,1]], device=device, dtype=dtype);\n",
    "    # (13)\n",
    "    Sym[2,:,:] = th.tensor([[0,0,1],[0,1,0],[1,0,0]], device=device, dtype=dtype);\n",
    "    # (23)\n",
    "    Sym[3,:,:] = th.tensor([[1,0,0],[0,0,1],[0,1,0]], device=device, dtype=dtype);\n",
    "    # (123)\n",
    "    Sym[4,:,:] = th.tensor([[0,1,0],[0,0,1],[1,0,0]], device=device, dtype=dtype);\n",
    "    # (132)\n",
    "    Sym[5,:,:] = th.tensor([[0,0,1],[1,0,0],[0,1,0]], device=device, dtype=dtype);\n",
    "\n",
    "    # coeff's\n",
    "    symc = th.tensor([4.0,4.0,-2.0,-2.0,-2.0,-2.0], device=device, dtype=dtype);\n",
    "\n",
    "    # Sample parameters should return energy of -7.3615\n",
    "    xvechL=th.tensor([\n",
    "         1.6210e+00, -2.1504e-01,  9.0755e-01,  9.7866e-01, -2.8418e-01,\n",
    "        -3.5286e+00, -3.3045e+00, -4.5036e+00, -3.2116e-01, -7.1901e-02,\n",
    "         1.5167e+00, -8.4489e-01, -2.1377e-01, -3.6127e-03, -5.3774e-03,\n",
    "        -2.1263e+00, -2.5191e-01,  2.1235e+00, -2.1396e-01, -1.4084e-03,\n",
    "        -1.0092e-02,  4.5349e+00,  9.4837e-03,  1.1225e+00, -2.1315e-01,\n",
    "         5.8451e-02, -4.9410e-03,  5.0853e+00,  7.3332e-01,  5.0672e+00,\n",
    "        -2.1589e-01, -6.8986e-03, -1.4310e-02,  1.5979e+00,  3.3946e-02,\n",
    "        -8.7965e-01, -1.1121e+00, -2.1903e-03, -4.6925e-02,  2.1457e-01,\n",
    "         3.3045e-03,  4.5120e+00, -2.1423e-01, -1.6493e-02, -2.3429e-03,\n",
    "        -8.6715e-01, -6.7070e-02,  1.5998e+00\n",
    "     ], device=device, dtype=dtype, requires_grad=False)\n",
    "\n",
    "    evec = th.tensor([\n",
    "      -6.0460e-02,  7.7708e-05, 1.6152e+00,  9.5443e-01,  \n",
    "      1.1771e-01,  3.2196e+00,  9.6344e-01, 3.1398e+00\n",
    "    ], device=device, dtype=dtype, requires_grad=False)\n",
    "\n",
    "    \n",
    "    # uncomment following lines to test above \n",
    "    #nb=8\n",
    "    #x1 = th.tensor(th.cat((xvechL,evec)), device=device, dtype=dtype, requires_grad=True)\n",
    "    #energy = b_energyrc(x1,n,nb,Mass,Charge,Sym,symc) \n",
    "    #print(energy) # should be -7.3615\n",
    "    #return x1\n",
    "    \n",
    "    if restart:\n",
    "        nb=num_basis\n",
    "        x1 = xrestart\n",
    "    else:\n",
    "        # random start point\n",
    "        nb=num_basis\n",
    "        #th.manual_seed(333)\n",
    "        x1 = th.empty(int(nb*n*(n+1)/2 + nb), device=device, dtype=dtype, requires_grad=True)\n",
    "        th.nn.init.uniform_(x1, a=-0.8, b=0.8)\n",
    "        \n",
    "    # start from a restart value\n",
    "    #x1 = xrestart\n",
    "    #print(energy)\n",
    "    #return x1\n",
    "    \n",
    "    # Do the Optimization\n",
    "    #optimizer = th.optim.LBFGS([x1])\n",
    "    #optimizer = th.optim.Adadelta([x1], lr=160.0)\n",
    "    #optimizer = th.optim.Adam([x1], lr=0.00005)\n",
    "    optimizer = th.optim.Rprop([x1], lr=0.0001, etas=(0.5, 1.2), step_sizes=(1e-07, 50))\n",
    "    \n",
    "    #scheduler = th.optim.lr_scheduler.ReduceLROnPlateau(optimizer,threshold=0.00001,cooldown=3, verbose=True,patience=2, factor=0.5)\n",
    "    \n",
    "    for i in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        loss = b_energyrc(x1,n,nb,Mass,Charge,Sym,symc)\n",
    "        loss.backward()\n",
    "        #def closure():\n",
    "        #    return b_energyrc(x1,n,nb,Mass,Charge,Sym,symc)\n",
    "        #optimizer.step(closure)\n",
    "        optimizer.step()\n",
    "        #scheduler.step(loss)\n",
    "        \n",
    "        if (i<20 or not i%100):print('step: {:5}  f: {:4.12f}  gradNorm: {:.9f}'.format(i, loss, th.norm(x1.grad)))\n",
    "    # print last value\n",
    "    print('step: {:5}  f: {:4.12f}  gradNorm: {:.9f}'.format(i, loss, th.norm(x1.grad)))\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tart_time = time.time()\n",
    "for i in range(25):\n",
    "    print(\"Optimization restart: {}\".format(i))\n",
    "    xrestart = opt_energyrc(steps=10000,num_basis=640, restart=True)\n",
    "print(\" took {:.4f} seconds \".format(time.time() - start_time))\n",
    "#cProfile.run('xprof = opt_energyrc()')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
