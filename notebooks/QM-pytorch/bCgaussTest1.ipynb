{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batched Cgauss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "import time\n",
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution device:  cuda:0\n",
      "PyTorch version:  0.4.1\n",
      "CUDA available:  True\n",
      "CUDA version:  9.0.176\n",
      "CUDA device: TITAN V\n"
     ]
    }
   ],
   "source": [
    "dtype = th.float64\n",
    "\n",
    "gpuid = 0\n",
    "device = th.device(\"cuda:\"+ str(gpuid))\n",
    "#device = th.device(\"cpu\")\n",
    "\n",
    "print(\"Execution device: \",device)\n",
    "print(\"PyTorch version: \", th.__version__ )\n",
    "print(\"CUDA available: \", th.cuda.is_available())\n",
    "print(\"CUDA version: \", th.version.cuda)\n",
    "print(\"CUDA device:\", th.cuda.get_device_name(gpuid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vech2L(v,n):\n",
    "    count = 0\n",
    "    L = th.zeros((n,n))\n",
    "    for j in range(n):\n",
    "        for i in range(j,n):\n",
    "            L[i,j]=v[count]\n",
    "            count = count + 1\n",
    "    return th.tensor(L , device=device, dtype=dtype)\n",
    "\n",
    "# batched vech2L input is \"X\" as V nb x n(n+1)/2\n",
    "def bvech2L(V,nb,n):\n",
    "    count = 0\n",
    "    L = th.zeros((nb,n,n))\n",
    "    for j in range(n):\n",
    "        for i in range(j,n):\n",
    "            L[...,i,j]=V[...,count]\n",
    "            count = count + 1\n",
    "    return th.tensor(L , device=device, dtype=dtype)\n",
    "\n",
    "def cholesky(A):\n",
    "    L = th.zeros_like(A)\n",
    "    \n",
    "    for i in range(A.shape[-1]):\n",
    "        for j in range(i+1):\n",
    "            s = 0.0\n",
    "            for k in range(j):\n",
    "                s = s + L[...,i,k].clone() * L[...,j,k].clone()\n",
    "            \n",
    "            L[...,i,j] = th.sqrt(A[...,i,i] - s) if (i == j) else \\\n",
    "                      (1.0 / L[...,j,j].clone() * (A[...,i,j] - s))\n",
    "    return L\n",
    "\n",
    "def inverseL(L):\n",
    "    n = L.shape[-1]\n",
    "    invL = th.zeros_like(L)\n",
    "    for j in range(0,n):\n",
    "        invL[...,j,j] = 1.0/L[...,j,j]\n",
    "        for i in range(j+1,n):\n",
    "            S = 0.0\n",
    "            for k in range(i+1):\n",
    "                S = S - L[...,i,k]*invL[...,k,j].clone()\n",
    "            invL[...,i,j] = S/L[...,i,i]\n",
    "\n",
    "    return invL\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_matel(n, Ak, Al, invAkl, detAkl, detLk, detLl, Mass, Qmat, Rkl):\n",
    "    \n",
    "    \n",
    "    # apply symmetry projection on Ll\n",
    "    \n",
    "    # th.t() is shorthand for th.transpose(X, 0,1)\n",
    "    #PLl = th.t(Sym) @ Ll;\n",
    "    \n",
    "    # build Ak, Al, Akl, invAkl, invAk, invAl\n",
    "\n",
    "    #Ak = Lk@th.t(Lk);\n",
    "    #Al = PLl@th.t(PLl);\n",
    "    #Akl = Ak+Al;\n",
    "    #print(Al)\n",
    "    #invAkl = th.inverse(Akl);\n",
    "    # Overlap: (normalized)\n",
    "    skl = 2**(n*1.5) * th.sqrt( th.pow(detLk*detLl/detAkl ,3) );\n",
    "\n",
    "    # kinetic energy\n",
    "    #print('Ak',Ak)\n",
    "    #print('Al',Al)\n",
    "    #print('invAkl', invAkl)\n",
    "    #print('Ak invAkl Al', Ak@invAkl@Al)\n",
    "    tkl = skl*(6*th.trace(Mass@Ak@invAkl@Al));    \n",
    "    #tkl = skl*(6*th.sum(Mass*(Ak@invAkl@Al)))\n",
    "    \n",
    "    # potential energy\n",
    "    \n",
    "    TWOoSqrtPI = 1.1283791670955126 # 2/sqrt(pi)\n",
    "    \n",
    "    #RIJ = th.zeros((n,n), device=device, dtype=dtype);\n",
    "    # 1/rij i~=j\n",
    "    #for j in range(0,n-1):\n",
    "    #    for i in range(j+1,n):\n",
    "    #        tmp2 = invAkl[i,i] + invAkl[j,j] - 2*invAkl[i,j];\n",
    "    #        #RIJ[i,j] = TWOoSqrtPI * skl/th.sqrt(tmp2);\n",
    "    #        RIJ[i,j] = th.rsqrt(tmp2)\n",
    "\n",
    "\n",
    "    # 1/rij i=j\n",
    "    #for i in range(0,n):\n",
    "        #RIJ[i,i] = TWOoSqrtPI * skl/th.sqrt(invAkl[i,i]);\n",
    "    #    RIJ[i,i] = th.rsqrt(invAkl[i,i])\n",
    "    \n",
    "    Rkl = TWOoSqrtPI*skl*Rkl\n",
    "    \n",
    "    #Q = vech2L(vecQ,n);\n",
    "\n",
    "    vkl = th.sum(Rkl*Qmat)\n",
    "\n",
    "    #hkl = tkl + vkl\n",
    "    \n",
    "    \n",
    "    return {'skl':skl, 'tkl':tkl, 'vkl':vkl}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_matel():\n",
    "    n = 3;\n",
    "    nb = 2\n",
    "    nn = int(n*(n+1)/2)\n",
    "    \n",
    "    vechLk = th.tensor([  1.00000039208682, \n",
    "              0.02548044275764261, \n",
    "              0.3525161612610669,\n",
    "              1.6669144815242515,\n",
    "              0.9630555318946559,\n",
    "              1.8382882034659822 ], device=device, dtype=dtype, requires_grad=True);\n",
    "    \n",
    "    vechLl = th.tensor([  1.3353550436464964,\n",
    "               0.9153272033682132,\n",
    "               0.7958636766525028,\n",
    "               1.8326931436447955,\n",
    "               0.3450426931160630,\n",
    "               1.8711839323167831 ], device=device, dtype=dtype, requires_grad=True);\n",
    "    \n",
    "    Sym = th.tensor([[0,0,1],\n",
    "                    [0,1,0],\n",
    "                    [1,0,0]], device=device, dtype=dtype);\n",
    "    \n",
    "    #Sym = th.tensor([[1,0,0],\n",
    "     #               [0,1,0],\n",
    "      #              [0,0,1]], device=device, dtype=dtype);\n",
    "    \n",
    "    Mass = th.tensor([[5.446170e-4, 2.723085077e-4, 2.723085077e-4],\n",
    "                     [2.723085077e-4, .5002723085, 2.723085077e-4],\n",
    "                     [2.723085077e-4, 2.723085077e-4, .5002723085 ]], device=device, dtype=dtype);\n",
    "    \n",
    "    Qmat = vech2L(th.tensor([1, -1, -1, -1, 1, -1], device=device, dtype=dtype),n);\n",
    "    \n",
    "    x = th.cat((vechLk,vechLl))\n",
    "    # reshape non-linear variables for easier indexing\n",
    "    X = th.reshape(x[:nb*nn], (nb,nn))\n",
    "    #npX = X.detach().numpy()\n",
    "    L = th.zeros((nb,n,n), device=device, dtype=dtype)\n",
    "    #for i in range(nb):\n",
    "    #    L[i][:,:] = vech2L(X[i,:],n)\n",
    "    L = bvech2L(X,nb,n)\n",
    "\n",
    "    detL = th.abs(th.prod(th.diagonal(L, offset=0, dim1=-1, dim2=-2),1))\n",
    "    AK = th.matmul(L,th.transpose(L, 1, 2))\n",
    "    #PL = th.matmul(th.t(Sym),L)\n",
    "    #AL = th.matmul(PL,th.transpose(PL, 1, 2))\n",
    "    AL = th.matmul(th.t(Sym), th.matmul(AK,Sym))\n",
    "    \n",
    "    AKL = th.zeros((nb,nb,n,n), device=device, dtype=dtype)\n",
    "    for i in range(nb):\n",
    "        for j in range(nb):\n",
    "            AKL[i,j] =  th.add(AK[i], AL[j]) \n",
    "    cholAKL = cholesky(AKL)\n",
    "    detAKL = th.prod(th.diagonal(th.reshape(cholAKL, (nb*nb,n,n)), offset=0, dim1=-1, dim2=-2),1)**2\n",
    "    #print(detAKL[0])\n",
    "    #detAKL *= detAKL\n",
    "    detAKL = th.reshape(detAKL, (nb,nb))\n",
    "    invLKL=inverseL(cholAKL)\n",
    "    #print(invLKL)\n",
    "    #print(th.transpose(invLKL, dim0=-2, dim1=-1))\n",
    "    invAKL = th.matmul(th.transpose(invLKL, dim0=-1, dim1=-2),invLKL)\n",
    "    #print(th.prod(th.diag(cholAKL[0,0])))\n",
    "    #print(th.prod(th.diag(th.potrf(AKL[0,0],upper=False))))\n",
    "    #print(th.prod(th.diagonal(cholAKL[0,0], offset=0, dim1=-1, dim2=-2),0))\n",
    "    \n",
    "    RIJ = th.zeros_like(invAKL, device=device, dtype=dtype);\n",
    "    # 1/rij i~=j\n",
    "    for j in range(0,n-1):\n",
    "        for i in range(j+1,n):\n",
    "            tmp2 = invAKL[...,i,i] + invAKL[...,j,j] - 2*invAKL[...,i,j];\n",
    "            #RIJ[i,j] = TWOoSqrtPI * skl/th.sqrt(tmp2);\n",
    "            RIJ[...,i,j] = th.rsqrt(tmp2)\n",
    "\n",
    "    # 1/rij i=j\n",
    "    for i in range(0,n):\n",
    "        #RIJ[i,i] = TWOoSqrtPI * skl/th.sqrt(invAkl[i,i]);\n",
    "        RIJ[...,i,i] = th.rsqrt(invAKL[...,i,i])    \n",
    "    \n",
    "    # Overlap: (normalized)\n",
    "    #print(th.ger(detL, detL)/detAKL)\n",
    "    SKL = 2**(n*1.5) * th.sqrt( th.pow(th.ger(detL, detL)/detAKL ,3) );\n",
    "    #print(SKL)\n",
    "    \n",
    "    # kinetic energy\n",
    "    #TKL= SKL*6*th.sum(Mass*th.matmul(AK, th.matmul(invAKL,AL)),(2,3))\n",
    "    \n",
    "    C = th.zeros_like(invAKL)\n",
    "    for i in range(nb):\n",
    "        for j in range(nb):\n",
    "            C[i,j] = (AK[i]@invAKL[i,j]@AL[j])\n",
    "    #C1 = th.matmul(invAKL,AL)\n",
    "    #C = th.matmul(AK, C1)\n",
    "    #C = th.matmul(invAKL,AL)\n",
    "    #print('bAK',AK)\n",
    "    #print('bAL',AL)\n",
    "    #print('binvAKL', invAKL)\n",
    "    #print('bAK invAKL AL', C)\n",
    "    #CT = th.transpose(C, dim0=-1, dim1=-2)\n",
    "    #TKL = 6*SKL*th.sum(th.diagonal(C, dim1=-2, dim2=-1), dim=-1, keepdim=False)\n",
    "    TKL = 6*SKL*th.sum(Mass*C, dim=(-2,-1))\n",
    "    \n",
    "    print(TKL)\n",
    "    #TKL = SKL*(6*th.trace(Mass@Ak@invAkl@Al))    \n",
    "    #tkl = skl*(6*th.sum(Mass*(Ak@invAkl@Al)))\n",
    "    \n",
    "    # potential energy\n",
    "    TWOoSqrtPI = 1.1283791670955126 # 2/sqrt(pi)\n",
    "    \n",
    "    VKL = TWOoSqrtPI*SKL*th.sum(RIJ*Qmat,dim=(-2,-1))\n",
    "    #print(VKL)\n",
    "\n",
    "    \n",
    "    for i in range(0,nb):\n",
    "        for j in range(i,nb):\n",
    "            Ai = AK[i,:,:]\n",
    "            Aj = AL[j,:,:]\n",
    "            Aij = AKL[i,j]\n",
    "            #print(Aj)\n",
    "            detLi = detL[i]\n",
    "            detLj = detL[j]\n",
    "            detAij = detAKL[i,j]\n",
    "            #detAij = th.det(Ai+Aj)\n",
    "            #print(detAij)\n",
    "            #print(th.det(Ai+Aj))\n",
    "            #print(Aij)\n",
    "            #print(Ai+Aj)\n",
    "            invAij = invAKL[i,j]\n",
    "            #print(i,j)\n",
    "            Rij = RIJ[i,j]\n",
    "            matels = b_matel(n, Ai, Aj, invAij, detAij, detLi, detLj, Mass, Qmat,Rij)\n",
    "    \n",
    "            print('skl: ',matels['skl'])\n",
    "            print('tkl: ',matels['tkl'])\n",
    "            print('vkl: ',matels['vkl'])\n",
    "    \n",
    "    #tkl = matels['tkl']\n",
    "    #print(th.autograd.grad(tkl, vechLk, retain_graph=True) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.3546, 4.3509],\n",
      "        [3.7564, 6.7999]], device='cuda:0', dtype=torch.float64, grad_fn=<ThMulBackward>)\n",
      "skl:  tensor(0.4115, device='cuda:0', dtype=torch.float64, grad_fn=<MulBackward>)\n",
      "tkl:  tensor(2.3546, device='cuda:0', dtype=torch.float64, grad_fn=<ThMulBackward>)\n",
      "vkl:  tensor(-1.7185, device='cuda:0', dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "skl:  tensor(0.5334, device='cuda:0', dtype=torch.float64, grad_fn=<MulBackward>)\n",
      "tkl:  tensor(4.3509, device='cuda:0', dtype=torch.float64, grad_fn=<ThMulBackward>)\n",
      "vkl:  tensor(-2.3840, device='cuda:0', dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "skl:  tensor(0.6856, device='cuda:0', dtype=torch.float64, grad_fn=<MulBackward>)\n",
      "tkl:  tensor(6.7999, device='cuda:0', dtype=torch.float64, grad_fn=<ThMulBackward>)\n",
      "vkl:  tensor(-3.0929, device='cuda:0', dtype=torch.float64, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_matel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_energyrc(x,n,nb,Mass,Qmat,Sym,symc):\n",
    "    \n",
    "    nx = len(x);\n",
    "    nn = int(n*(n+1)/2);\n",
    "    nsym = len(symc);\n",
    "    #nsym = 1\n",
    "    \n",
    "    # extract linear coefs \"eigen vector\"\n",
    "    c = x[-nb:];\n",
    "    # reshape non-linear variables for easier indexing\n",
    "    X = th.reshape(x[:nb*nn], (nb,nn))\n",
    "    #npX = X.detach().numpy()\n",
    "    L = th.zeros((nb,n,n), device=device, dtype=dtype)\n",
    "\n",
    "    L = bvech2L(X,nb,n)\n",
    "\n",
    "    detL = th.abs(th.prod(th.diagonal(L, offset=0, dim1=-1, dim2=-2),1))\n",
    "    AK = th.matmul(L,th.transpose(L, 1, 2))\n",
    "\n",
    "    \n",
    "    # Build H and S\n",
    "    H = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    S = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    T = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    V = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    \n",
    "    # Build H and S\n",
    "    bH = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    bS = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    bT = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    bV = th.zeros((nb,nb), device=device, dtype=dtype);\n",
    "    \n",
    "    # outer loop is over symmetry terms\n",
    "    for k in range(0,nsym):\n",
    "        \n",
    "        P = Sym[:,:,k]\n",
    "\n",
    "        AL = th.matmul(th.t(P), th.matmul(AK,P))\n",
    "\n",
    "        AKL = th.zeros((nb,nb,n,n), device=device, dtype=dtype)\n",
    "        for i in range(nb):\n",
    "            for j in range(nb):\n",
    "                AKL[i,j] =  th.add(AK[i], AL[j]) \n",
    "        cholAKL = cholesky(AKL)\n",
    "        #cholAKL = th.zeros_like(AKL)\n",
    "        #for i in range(nb):\n",
    "        #    for j in range(nb):\n",
    "        #        cholAKL[i,j] = th.potrf(AKL[i,j], upper=False) \n",
    "        \n",
    "        detAKL = th.prod(th.diagonal(cholAKL, offset=0, dim1=-1, dim2=-2),-1)**2\n",
    "        #print(detAKL[0])\n",
    "        #detAKL *= detAKL\n",
    "        #detAKL = th.reshape(detAKL, (nb,nb))\n",
    "        invLKL = inverseL(cholAKL)\n",
    "        #invLKL = th.zeros_like(AKL)\n",
    "        #for i in range(nb):\n",
    "        #    for j in range(nb):\n",
    "        #        invLKL[i,j] = th.inverse(cholAKL[i,j]) \n",
    "\n",
    "\n",
    "        invAKL = th.matmul(th.transpose(invLKL, dim0=-1, dim1=-2),invLKL)\n",
    "\n",
    "        RIJ = th.zeros_like(invAKL, device=device, dtype=dtype);\n",
    "        # 1/rij i~=j\n",
    "        for j in range(0,n-1):\n",
    "            for i in range(j+1,n):\n",
    "                tmp2 = invAKL[...,i,i] + invAKL[...,j,j] - 2*invAKL[...,i,j];\n",
    "                #RIJ[i,j] = TWOoSqrtPI * skl/th.sqrt(tmp2);\n",
    "                RIJ[...,i,j] = th.rsqrt(tmp2)\n",
    "\n",
    "        # 1/rij i=j\n",
    "        for i in range(0,n):\n",
    "            #RIJ[i,i] = TWOoSqrtPI * skl/th.sqrt(invAkl[i,i]);\n",
    "            RIJ[...,i,i] = th.rsqrt(invAKL[...,i,i])    \n",
    "\n",
    "\n",
    "        # Overlap: (normalized)\n",
    "        #print(th.ger(detL, detL)/detAKL)\n",
    "        SKL = 2**(n*1.5) * th.sqrt( th.pow(th.ger(detL, detL)/detAKL ,3) );\n",
    "        #print(symc[k]*SKL)\n",
    "\n",
    "        # kinetic energy\n",
    "        C = th.zeros_like(invAKL)\n",
    "        for i in range(nb):\n",
    "            for j in range(nb):\n",
    "                C[i,j] = (AK[i]@invAKL[i,j]@AL[j])\n",
    "        #C = th.matmul(AK, th.matmul(invAKL,AL))\n",
    "        #TKL = 6*SKL*th.sum(th.diagonal(C, dim1=-2, dim2=-1), dim=-1, keepdim=False)\n",
    "        TKL = 6*SKL*th.sum(Mass*C, dim=(-2,-1))\n",
    "        #print(symc[k]*TKL)\n",
    "        #TKL = SKL*(6*th.trace(Mass@Ak@invAkl@Al))    \n",
    "        #tkl = skl*(6*th.sum(Mass*(Ak@invAkl@Al)))\n",
    "\n",
    "        # potential energy\n",
    "        TWOoSqrtPI = 1.1283791670955126 # 2/sqrt(pi)\n",
    "        \n",
    "        VKL = TWOoSqrtPI*SKL*th.sum(RIJ*Qmat, dim=(-2,-1))\n",
    "        #print(symc[k]*VKL)\n",
    "    \n",
    "            \n",
    "        #for i in range(0,nb):\n",
    "        #    for j in range(i,nb):\n",
    "        #        Ai = AK[i,:,:]\n",
    "        #        Aj = AL[j,:,:]\n",
    "        #        Aij = AKL[i,j]\n",
    "        #        detLi = detL[i]\n",
    "        #        detLj = detL[j]\n",
    "        #        detAij = detAKL[i,j]\n",
    "        #        invAij = invAKL[i,j]\n",
    "        #        Rij = RIJ[i,j]\n",
    "        #        matels = b_matel(n, Ai, Aj, invAij, detAij, detLi, detLj, Mass, Qmat,Rij)\n",
    "        #        S[i,j] = S[i,j] + symc[k]*matels['skl'];\n",
    "        #        T[i,j] = T[i,j] + symc[k]*matels['tkl'];\n",
    "        #        V[i,j] = V[i,j] + symc[k]*matels['vkl'];\n",
    "        \n",
    "        S = S + symc[k]*SKL\n",
    "        T = T + symc[k]*TKL\n",
    "        V = V + symc[k]*VKL\n",
    "        \n",
    "        #print(bT)\n",
    "        #print(T)\n",
    "    \n",
    "    H = T + V\n",
    "    #dH = dT + dV\n",
    "    \n",
    "    # complete upper triangle of H and S\n",
    "    for i in range(0,nb):\n",
    "        for j in range(i+1,nb):\n",
    "            H[j,i] = H[i,j]\n",
    "            S[j,i] = S[i,j]\n",
    "            #H[i,j] = H[j,i];\n",
    "            #S[i,j] = S[j,i];\n",
    "    th.set_printoptions(linewidth=120)        \n",
    "    #print(V)\n",
    "    cHc = c@H@c;\n",
    "    cSc = c@S@c;\n",
    "    eng = cHc/cSc;\n",
    "    \n",
    "    return eng           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_energyrc():\n",
    "        \n",
    "    n=3;\n",
    "    nb=8;\n",
    "    \n",
    "    Mass = th.tensor([[0.5, 0.0, 0.0],\n",
    "                     [0.0, 0.5, 0.0],\n",
    "                     [0.0, 0.0, 0.5]], device=device, dtype=dtype);\n",
    "    \n",
    "    Charge = th.tensor([-3, 1, 1, -3, 1, -3], device=device, dtype=dtype);\n",
    "    Charge = vech2L(Charge,n)\n",
    "    \n",
    "    # symmetry projection terms\n",
    "    Sym = th.zeros((3,3,6), device=device, dtype=dtype)\n",
    "    # (1)(2)(3)\n",
    "    Sym[:,:,0] = th.tensor([[1,0,0],[0,1,0],[0,0,1]], device=device, dtype=dtype);\n",
    "    # (12)\n",
    "    Sym[:,:,1] = th.tensor([[0,1,0],[1,0,0],[0,0,1]], device=device, dtype=dtype);\n",
    "    # (13)\n",
    "    Sym[:,:,2] = th.tensor([[0,0,1],[0,1,0],[1,0,0]], device=device, dtype=dtype);\n",
    "    # (23)\n",
    "    Sym[:,:,3] = th.tensor([[1,0,0],[0,0,1],[0,1,0]], device=device, dtype=dtype);\n",
    "    # (123)\n",
    "    Sym[:,:,4] = th.tensor([[0,1,0],[0,0,1],[1,0,0]], device=device, dtype=dtype);\n",
    "    # (132)\n",
    "    Sym[:,:,5] = th.tensor([[0,0,1],[1,0,0],[0,1,0]], device=device, dtype=dtype);\n",
    "\n",
    "    # coeff's\n",
    "    symc = th.tensor([4.0,4.0,-2.0,-2.0,-2.0,-2.0], device=device, dtype=dtype);\n",
    "\n",
    "    \n",
    "    xvechL=th.tensor([\n",
    "     1.6210e+00,\n",
    "    -2.1504e-01,\n",
    "     9.0755e-01,\n",
    "     9.7866e-01,\n",
    "    -2.8418e-01,\n",
    "    -3.5286e+00,\n",
    "    -3.3045e+00,\n",
    "    -4.5036e+00,\n",
    "    -3.2116e-01,\n",
    "    -7.1901e-02,\n",
    "     1.5167e+00,\n",
    "    -8.4489e-01,\n",
    "    -2.1377e-01,\n",
    "    -3.6127e-03,\n",
    "    -5.3774e-03,\n",
    "    -2.1263e+00,\n",
    "    -2.5191e-01,\n",
    "     2.1235e+00,\n",
    "    -2.1396e-01,\n",
    "    -1.4084e-03,\n",
    "    -1.0092e-02,\n",
    "     4.5349e+00,\n",
    "     9.4837e-03,\n",
    "     1.1225e+00,\n",
    "    -2.1315e-01,\n",
    "     5.8451e-02,\n",
    "    -4.9410e-03,\n",
    "     5.0853e+00,\n",
    "     7.3332e-01,\n",
    "     5.0672e+00,\n",
    "    -2.1589e-01,\n",
    "    -6.8986e-03,\n",
    "    -1.4310e-02,\n",
    "     1.5979e+00,\n",
    "     3.3946e-02,\n",
    "    -8.7965e-01,\n",
    "    -1.1121e+00,\n",
    "    -2.1903e-03,\n",
    "    -4.6925e-02,\n",
    "     2.1457e-01,\n",
    "     3.3045e-03,\n",
    "     4.5120e+00,\n",
    "    -2.1423e-01,\n",
    "    -1.6493e-02,\n",
    "    -2.3429e-03,\n",
    "    -8.6715e-01,\n",
    "    -6.7070e-02,\n",
    "     1.5998e+00\n",
    "     ], device=device, dtype=dtype, requires_grad=False)\n",
    "    \n",
    "    evec = th.tensor([\n",
    "      -6.0460e-02,\n",
    "       7.7708e-05,\n",
    "       1.6152e+00,\n",
    "       9.5443e-01,\n",
    "       1.1771e-01,\n",
    "       3.2196e+00,\n",
    "       9.6344e-01,\n",
    "       3.1398e+00\n",
    "    ], device=device, dtype=dtype, requires_grad=False)\n",
    "    \n",
    "    #x1 = th.tensor(th.cat((xvechL,evec)), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "    #energy = b_energyrc(x1,n,nb,Mass,Charge,Sym,symc)\n",
    "    #print(energy)\n",
    "    #return x1\n",
    "    \n",
    "    n=3;\n",
    "    nb=256;\n",
    "    #th.manual_seed(42)\n",
    "    #x1 = th.randn(int(nb*n*(n+1)/2 + nb) , device=device, dtype=dtype, requires_grad=True)\n",
    "    x1 = xrestart\n",
    "    #print(x1)\n",
    "    #energy, G = py_energyrc(x1,n,nb,Mass,Charge,Sym,symc)\n",
    "    #energy = b_energyrc(x1,n,nb,Mass,Charge,Sym,symc)\n",
    "    #print(energy)\n",
    "    #return x1\n",
    "    \n",
    "    #optimizer = th.optim.LBFGS([x1])\n",
    "    optimizer = th.optim.Adadelta([x1], lr=2.0)\n",
    "    #optimizer = th.optim.Adam([x1], lr=0.5)\n",
    "    \n",
    "    scheduler = th.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True,patience=2, factor=0.5)\n",
    "    \n",
    "    for i in range(1):\n",
    "        optimizer.zero_grad()\n",
    "        loss = b_energyrc(x1,n,nb,Mass,Charge,Sym,symc)\n",
    "        loss.backward()\n",
    "        #def closure():\n",
    "        #    return b_energyrc(x1,n,nb,Mass,Charge,Sym,symc)\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        \n",
    "        print('step: {} f: {} gradNorm: {}'.format(i, loss, th.norm(x1.grad)))\n",
    "    \n",
    "    return x1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 f: -7.442151726305396 gradNorm: 0.18193230712624947\n",
      " took 164.13843297958374 seconds \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "xrestart = test_energyrc()\n",
    "print(\" took {} seconds \".format(time.time() - start_time))\n",
    "#cProfile.run('xrestart = test_energyrc()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    " th.save(xrestart, 'Libo-nb256-7.442.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrestart = th.load('Libo-nb64-7.450.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
